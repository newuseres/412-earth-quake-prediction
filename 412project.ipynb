{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The first several cells are used for some common use like loading data.\n",
        "\n",
        "Link for kaggle competition:\n",
        "https://www.kaggle.com/competitions/ml-olympiad-predicting-earthquake-damage/data?select=train.csv\n",
        "\n",
        "If you have any idea, you can type it on your cells.\n",
        "\n",
        "In case Your code is overwritten, you can also write the code locally then copy and paste on there. This one is mainly for sharing and combining together."
      ],
      "metadata": {
        "id": "nbUUwzcXkjpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pytorch as py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "train_df = pd.read_csv(\"data/train.csv\")\n",
        "test_df = pd.read_csv(\"data/test.csv\")\n"
      ],
      "metadata": {
        "id": "yUsjhfVWkk03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vcWaccJcFDfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next: cell\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "ICwTzqKalYuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "19bMsonqsKof"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0nyVVxiLsKkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "arqe1Ae9nGQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e9oTRhsIlSce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRCbyesfg-J5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jinhao Luo parts\n",
        "The cells below are for Jinhao Luo:\n",
        "some kind of neural networks"
      ],
      "metadata": {
        "id": "JKwrdWghlrQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import scale\n",
        "from sklearn.discriminant_analysis import StandardScaler\n",
        "from sklearn.utils import compute_class_weight\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score, recall_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# load data\n",
        "train_df = pd.read_csv(\"data/train.csv\")\n",
        "test_df = pd.read_csv(\"data/test.csv\")\n",
        "test_building_id = test_df[\"building_id\"].copy()\n",
        "\n",
        "\n",
        "# deal with categorical features\n",
        "cat_cols = train_df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "for col in cat_cols:\n",
        "    train_df[col] = train_df[col].astype(\"category\").cat.codes\n",
        "    test_df[col] = test_df[col].astype(\"category\").cat.codes\n",
        "\n",
        "# prepare for features and labels\n",
        "X = train_df.drop([\"building_id\", \"damage_grade\"], axis=1).values\n",
        "Y = train_df[\"damage_grade\"].values.astype(int) - 1\n",
        "\n",
        "X_test = test_df.drop([\"building_id\"], axis=1).values\n",
        "\n",
        "# Normalize for data -- (ljh key change point)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# compute for weights for classes -- to deal with the problem of class's unbalance\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(Y), y=Y)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "\"\"\"\n",
        "# define our model\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),  # continue to lower dropout bility\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(\n",
        "                16, 3\n",
        "            ),  # output layer will be 3 , because damage_grade is a 3-class classification task\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# \"\"\" My net before\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(\n",
        "                16, 3\n",
        "            ),  # output layer will be 3 , because damage_grade is a 3-class classification task\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# cross validation\n",
        "fold_num = 5\n",
        "epoch_num = 100\n",
        "kfold = StratifiedKFold(n_splits=fold_num, shuffle=True, random_state=42)\n",
        "accuracies = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, Y)):\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    Y_train, Y_val = Y[train_idx], Y[val_idx]\n",
        "\n",
        "    train_dataset = TensorDataset(\n",
        "        torch.tensor(X_train, dtype=torch.float32),\n",
        "        torch.tensor(Y_train, dtype=torch.long),\n",
        "    )\n",
        "    val_dataset = TensorDataset(\n",
        "        torch.tensor(X_val, dtype=torch.float32), torch.tensor(Y_val, dtype=torch.long)\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    # initialize model, criterion and optimizer\n",
        "    model = MyNet(X_train.shape[1]).to(device)\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)  # Based on weighted class\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    best_model = None\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    # parameters for early stopping\n",
        "    patience = 40  # when validation loss doesn't improve for 10 epochs, stop training\n",
        "    no_improve_coutners = 0  # counter\n",
        "    delta = 0.001  # minimum change in validation loss to qualify as an improvement\n",
        "    early_stop = False  # sign for early stopping\n",
        "\n",
        "    # training loop\n",
        "    for epoch in range(epoch_num):\n",
        "        model.train()\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds = []\n",
        "        val_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                loss = criterion(pred, yb)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # get predicted features\n",
        "                _, predicted = torch.max(pred, 1)\n",
        "                val_preds.extend(predicted.cpu().numpy())\n",
        "                val_targets.extend(yb.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = accuracy_score(val_targets, val_preds)\n",
        "        val_recall = recall_score(val_targets, val_preds, average=\"macro\")\n",
        "        val_f1 = f1_score(val_targets, val_preds, average=\"macro\")\n",
        "\n",
        "        if val_loss < best_val_loss - delta:  # validation loss have enough improvement\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model.state_dict()\n",
        "            # save best model\n",
        "            torch.save(best_model, f\"best_model_fold_{fold}.pth\")\n",
        "            no_improve_coutners = 0  # reset counter\n",
        "        else:\n",
        "            no_improve_coutners += 1  # increment counter\n",
        "\n",
        "            if no_improve_coutners >= patience:  # no improvement for 'patience' epochs\n",
        "                early_stop = True\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        print(\n",
        "            f\"Fold {fold+1}, Epoch {epoch+1}, Val Loss: {val_loss:.4f}, \"\n",
        "            f\"Val Accuracy: {val_acc:.4f}, Val Recall: {val_recall:.4f}, Val F1: {val_f1:.4f}\"\n",
        "        )\n",
        "\n",
        "    accuracies.append(val_acc)\n",
        "\n",
        "mean_acc = np.mean(accuracies)\n",
        "print(f\"Mean Accuracy: {mean_acc:.4f}\")\n",
        "\n",
        "# Use best model\n",
        "best_model = MyNet(X.shape[1]).to(device)\n",
        "best_model.load_state_dict(torch.load(f\"best_model_fold_{fold_num-1}.pth\"))\n",
        "best_model.eval()\n",
        "\n",
        "# predict on train set\n",
        "X_full_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "with torch.no_grad():\n",
        "    y_pred_logits = best_model(X_full_tensor)\n",
        "    _, y_pred_full = torch.max(y_pred_logits, 1)\n",
        "\n",
        "y_pred_full = y_pred_full.cpu().numpy()\n",
        "\n",
        "# give a measurement on our NN\n",
        "print(\"Accuracy on full training set:\", accuracy_score(Y, y_pred_full))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(Y, y_pred_full))\n",
        "recall = recall_score(Y, y_pred_full, average=\"macro\")\n",
        "f1 = f1_score(Y, y_pred_full, average=\"macro\")\n",
        "print(f\"\\nMacro Recall on full training set: {recall:.4f}\")\n",
        "print(f\"Macro F1-score on full training set: {f1:.4f}\")\n",
        "\n",
        "# predict on test set\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "with torch.no_grad():\n",
        "    y_pred_logits = best_model(X_test_tensor)\n",
        "    _, y_pred_test = torch.max(y_pred_logits, 1)\n",
        "\n",
        "y_pred_test = y_pred_test.cpu().numpy()\n",
        "\n",
        "\n",
        "# Save submission\n",
        "y_pred_test = y_pred_test + 1\n",
        "submission = pd.DataFrame(\n",
        "    {\"building_id\": test_building_id, \"damage_grade\": y_pred_test}\n",
        ")\n",
        "submission.to_csv(\"data/submission.csv\", index=False)\n",
        "print(\"\\nYes Submission saved to 'data/submission.csv'\")\n"
      ],
      "metadata": {
        "id": "7s0L1EHQmZUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before the improvement\n",
        "\n",
        "network\n",
        "\n",
        "```python\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(\n",
        "                16, 3\n",
        "            ),  # output layer will be 3 , because damage_grade is a 3-class classification task\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "outcome\n",
        "\n",
        "```\n",
        "Mean Accuracy: 0.5382\n",
        "Accuracy on full training set: 0.5465\n",
        "Classification Report:\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "           0       0.66      0.55      0.60       729\n",
        "           1       0.53      0.91      0.67      1968\n",
        "           2       0.00      0.00      0.00      1303\n",
        "\n",
        "    accuracy                           0.55      4000\n",
        "   macro avg       0.39      0.49      0.42      4000\n",
        "weighted avg       0.38      0.55      0.44      4000\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "w9bK4flLvPN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## After the 1st improvement\n",
        "\n",
        "\n",
        "\n",
        "I add the\n",
        "\n",
        "1. \\# compute for weights for classes -- to deal with the problem of class's unbalance\n",
        "\n",
        "   ```python\n",
        "   class_weights = compute_class_weight(\"balanced\", classes=np.unique(Y), y=Y)\n",
        "   class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "   ```\n",
        "\n",
        "   \n",
        "\n",
        "2. \\# Normalize for data -- (ljh key change point)\n",
        "\n",
        "   ```python\n",
        "   scaler = StandardScaler()\n",
        "   \n",
        "   X = scaler.fit_transform(X)\n",
        "   \n",
        "   X_test = scaler.transform(X_test)\n",
        "   ```\n",
        "\n",
        "   \n",
        "\n",
        "3. Change the network ( it seems a little failed)\n",
        "\n",
        "```python\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),  # continue to lower dropout bility\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(\n",
        "                16, 3\n",
        "            ),  # output layer will be 3 , because damage_grade is a 3-class classification task\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "```\n",
        "\n",
        "outcome\n",
        "\n",
        "```\n",
        "Mean Accuracy: 0.5382\n",
        "Accuracy on full training set: 0.5465\n",
        "\n",
        "Mean Accuracy: 0.4682\n",
        "Accuracy on full training set: 0.476\n",
        "\n",
        "Classification Report:\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "           0       0.51      0.78      0.61       729\n",
        "           1       0.65      0.09      0.16      1968\n",
        "           2       0.44      0.88      0.59      1303\n",
        "\n",
        "    accuracy                           0.48      4000\n",
        "   macro avg       0.53      0.59      0.46      4000\n",
        "weighted avg       0.55      0.48      0.39      4000\n",
        "```\n",
        "\n",
        "It's sad that the outcome become less, So I try to change the early stop longer first. (from delta 0.001 and patience 20 to (0.001,40), and also roll back to last net structure.\n",
        "\n",
        "#### change back to before net and patience to 40\n",
        "\n",
        "outcome\n",
        "\n",
        "```\n",
        "Mean Accuracy: 0.4813\n",
        "Accuracy on full training set: 0.475\n",
        "\n",
        "Classification Report:\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "           0       0.53      0.77      0.63       729\n",
        "           1       0.61      0.10      0.17      1968\n",
        "           2       0.44      0.88      0.58      1303\n",
        "\n",
        "    accuracy                           0.47      4000\n",
        "   macro avg       0.53      0.58      0.46      4000\n",
        "weighted avg       0.54      0.47      0.39      4000\n",
        "```\n",
        "\n",
        "#### Cancel the normalization?\n",
        "\n",
        "outcome\n",
        "\n",
        "```\n",
        "Mean Accuracy: 0.4788\n",
        "Accuracy on full training set: 0.479\n",
        "\n",
        "Classification Report:\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "           0       0.53      0.76      0.63       729\n",
        "           1       0.59      0.12      0.20      1968\n",
        "           2       0.44      0.87      0.58      1303\n",
        "\n",
        "    accuracy                           0.48      4000\n",
        "   macro avg       0.52      0.58      0.47      4000\n",
        "weighted avg       0.53      0.48      0.40      4000\n",
        "```\n",
        "\n",
        "So I may need more experiment on that to get a better score.\n",
        "\n"
      ],
      "metadata": {
        "id": "39JQN1FvVruJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Q33RIZDmZpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aUU_0BX9mZ7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Denis Chen parts\n",
        "The cells below are for Denis Chen"
      ],
      "metadata": {
        "id": "BzsMb_Gemaso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn import metrics\n",
        "\n",
        "if(len(sys.argv) < 2):\n",
        "    print(\"Usage: earthquake_prediction.py <training_data> [test_data | cross_validation_test_size] [write_to_csv]\")\n",
        "    sys.exit()\n",
        "\n",
        "input = pd.read_csv(sys.argv[1])\n",
        "X = input[[\"has_superstructure_adobe_mud\",\"has_superstructure_mud_mortar_stone\",\"has_superstructure_stone_flag\",\"has_superstructure_cement_mortar_stone\",\"has_superstructure_mud_mortar_brick\",\"has_superstructure_cement_mortar_brick\",\"has_superstructure_timber\",\"has_superstructure_bamboo\",\"has_superstructure_rc_non_engineered\",\"has_superstructure_rc_engineered\",\"has_superstructure_other\",\"has_secondary_use\",\"has_secondary_use_agriculture\",\"has_secondary_use_hotel\",\"has_secondary_use_rental\",\"has_secondary_use_institution\",\"has_secondary_use_school\",\"has_secondary_use_industry\",\"has_secondary_use_health_post\",\"has_secondary_use_gov_office\",\"has_secondary_use_use_police\",\"has_secondary_use_other\"]].to_numpy()\n",
        "y = input[[\"damage_grade\"]].to_numpy().reshape(-1)\n",
        "\n",
        "write_csv = False\n",
        "cross_validation = -1\n",
        "\n",
        "if(len(sys.argv) >= 4 and sys.argv[3] == \"write\"):\n",
        "    write_csv = True\n",
        "\n",
        "if(os.path.exists(sys.argv[2])):\n",
        "    test = pd.read_csv(sys.argv[2])\n",
        "else:\n",
        "    cross_validation = float(sys.argv[2])\n",
        "\n",
        "if(cross_validation == -1):\n",
        "    X_train = X\n",
        "    y_train = y\n",
        "    X_test = test[[\"has_superstructure_adobe_mud\",\"has_superstructure_mud_mortar_stone\",\"has_superstructure_stone_flag\",\"has_superstructure_cement_mortar_stone\",\"has_superstructure_mud_mortar_brick\",\"has_superstructure_cement_mortar_brick\",\"has_superstructure_timber\",\"has_superstructure_bamboo\",\"has_superstructure_rc_non_engineered\",\"has_superstructure_rc_engineered\",\"has_superstructure_other\",\"has_secondary_use\",\"has_secondary_use_agriculture\",\"has_secondary_use_hotel\",\"has_secondary_use_rental\",\"has_secondary_use_institution\",\"has_secondary_use_school\",\"has_secondary_use_industry\",\"has_secondary_use_health_post\",\"has_secondary_use_gov_office\",\"has_secondary_use_use_police\",\"has_secondary_use_other\"]].to_numpy()\n",
        "    y_test = np.array([])\n",
        "else:\n",
        "    print(\"Cross Validation:\" + str(cross_validation))\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = cross_validation, random_state = 0)\n",
        "\n",
        "print(X_train.shape)\n",
        "clf = BernoulliNB()\n",
        "clf.fit(X_train, y_train)\n",
        "prediction = clf.predict(X_test)\n",
        "\n",
        "#print(prediction.reshape((-1,1)).shape)\n",
        "#print(test[\"building_id\"].to_numpy().reshape((-1,1)).shape)\n",
        "\n",
        "if(write_csv):\n",
        "    print(\"Predictions written to csv\")\n",
        "    output = np.concatenate(((X_test[\"building_id\"].to_numpy()).reshape((-1,1)), prediction.reshape((-1,1))), axis = 1).astype(int)\n",
        "    np.savetxt(\"output.csv\", output, fmt='%d', delimiter = \",\", header = \"building_id,damage_grade\", comments='')\n",
        "\n",
        "if(y_test.size > 0):\n",
        "    print(metrics.accuracy_score(y_test, prediction))\n",
        "    print(metrics.f1_score(y_test, prediction, average='macro'))"
      ],
      "metadata": {
        "id": "IO2hfuU2memE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RGQ6VhYCtdbw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-8CaD87nmeym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7lUs36hprnPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gp6ft4VgnA5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yicheng Ge parts\n",
        "The cells below are for Yicheng Ge"
      ],
      "metadata": {
        "id": "vy90RqrCnAGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "train_data = pd.read_csv('/data/train.csv')\n",
        "test_data = pd.read_csv('/data/test.csv')\n",
        "test_object_columns = test_data.select_dtypes(include=['object']).columns\n",
        "test_data_encoded = pd.get_dummies(test_data, columns=test_object_columns)\n",
        "test_X = test_data_encoded.drop(['building_id'], axis=1)\n",
        "\n",
        "object_columns = train_data.select_dtypes(include=['object']).columns\n",
        "train_data_encoded = pd.get_dummies(train_data, columns=object_columns)\n",
        "X = train_data_encoded.drop(['building_id', 'damage_grade'], axis=1)\n",
        "y = train_data_encoded['damage_grade']\n",
        "\n",
        "# åˆ›å»ºçº¿æ€§å›žå½’æ¨¡åž‹å¯¹è±¡\n",
        "#---------train on old data------------\n",
        "model = LinearRegression()\n",
        "\n",
        "# ä½¿ç”¨è®­ç»ƒé›†è®­ç»ƒæ¨¡åž‹\n",
        "model.fit(X, y)\n",
        "\n",
        "# åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œé¢„æµ‹\n",
        "y_pred = model.predict(test_X)\n",
        "y_pred = y_pred.astype(int)\n",
        "\n",
        "# æŸ¥çœ‹æ¨¡åž‹ç³»æ•°\n",
        "coefficients = model.coef_\n",
        "\n",
        "# æŸ¥çœ‹æ¨¡åž‹æˆªè·\n",
        "intercept = model.intercept_\n",
        "\n",
        "print(f\"æ¨¡åž‹ç³»æ•°: {coefficients}\")\n",
        "print(f\"æ¨¡åž‹æˆªè·: {intercept}\")\n",
        "\n",
        "# åˆ›å»ºæäº¤ç»“æžœçš„DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    \"building_id\": test_data['building_id'],\n",
        "    \"damage_grade\": y_pred\n",
        "})\n",
        "\n",
        "# ä¿å­˜ä¸ºcsvæ–‡ä»¶\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(f\"\\nâœ… Submission saved to:'submission.csv'\")\n"
      ],
      "metadata": {
        "id": "SykN-UPAWQ2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc12cff-15df-4579-a794-0d7d2664585c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ¨¡åž‹ç³»æ•°: [-0.01160897  0.00264301 -0.00355638  0.00576985  0.05712092  0.21299997\n",
            "  0.0008602  -0.03937349  0.03187101 -0.17235406  0.02542759  0.04636667\n",
            " -0.15564023 -0.07583848 -0.13285055 -0.00377469 -0.01548817 -0.02490501\n",
            " -0.15078038 -0.10927813 -0.2989145  -0.19309835  0.04005061 -0.05471301\n",
            "  0.00948364 -0.06299941 -0.02386116  0.0118964   0.00440632 -0.01630272\n",
            "  0.06814449 -0.19442008  0.14848146 -0.06953992  0.04733404  0.04594458\n",
            "  0.035885   -0.08182958  0.00543534  0.01347879 -0.08081396  0.00174775\n",
            "  0.06015208 -0.06067713  0.03797399 -0.04195916  0.0646623  -0.05458788\n",
            "  0.1168224  -0.0257731  -0.03646142  0.02278153 -0.08290071  0.12222928\n",
            " -0.25431575  0.02663659 -0.18192662 -0.18538101  0.10935821  0.37020827\n",
            "  0.05331021  0.06065161 -0.11999117  0.02347214  0.03586742]\n",
            "æ¨¡åž‹æˆªè·: 1.8501449654939115\n",
            "\n",
            "âœ… Submission saved to:'submission.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "#---------train after improvement----------\n",
        "train_data_without_id = train_data_encoded.drop(['building_id'], axis=1)#data set without id\n",
        "full_correlation_matrix = train_data_without_id.corr().round(2)\n",
        "target_corr = full_correlation_matrix['damage_grade'].drop('damage_grade')\n",
        "#select the data correlation bigger than 0.25\n",
        "selected_features = target_corr[abs(target_corr) > 0.25].index.tolist()\n",
        "\n",
        "# ä»Ž X_train ä¸­é€‰å–é€‰å®šç‰¹å¾çš„æ•°æ®ï¼Œå¾—åˆ°æ–°çš„è®­ç»ƒé›†ç‰¹å¾æ•°æ®\n",
        "X_train_selected = X[selected_features]\n",
        "X_test_selected = test_data_encoded[selected_features]\n",
        "\n",
        "# åˆ›å»ºçº¿æ€§å›žå½’æ¨¡åž‹å¯¹è±¡\n",
        "model = LinearRegression()\n",
        "\n",
        "# ä½¿ç”¨è®­ç»ƒé›†è®­ç»ƒæ¨¡åž‹\n",
        "model.fit(X_train_selected, y)\n",
        "\n",
        "y_pred = model.predict(X_test_selected)\n",
        "y_pred = y_pred.astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# æŸ¥çœ‹æ¨¡åž‹ç³»æ•°\n",
        "coefficients = model.coef_\n",
        "\n",
        "# æŸ¥çœ‹æ¨¡åž‹æˆªè·\n",
        "intercept = model.intercept_\n",
        "\n",
        "print(f\"æ¨¡åž‹ç³»æ•°: {coefficients}\")\n",
        "print(f\"æ¨¡åž‹æˆªè·: {intercept}\")\n",
        "submission = pd.DataFrame({\n",
        "    \"building_id\": test_data['building_id'],\n",
        "    \"damage_grade\": y_pred\n",
        "})\n",
        "\n",
        "# ä¿å­˜ä¸ºcsvæ–‡ä»¶\n",
        "submission.to_csv('submission_afterimprove.csv', index=False)\n",
        "\n",
        "print(f\"\\nâœ… Submission saved to:'submission_afterimprove.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8dd2b1a-80e8-4c35-f3d9-66e56cb3b582",
        "id": "lp_rJ-13LBKh"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ¨¡åž‹ç³»æ•°: [ 0.00330215 -0.00612577  0.20952507 -0.21769286 -0.10686228 -0.18014633\n",
            "  0.15572031 -0.13446457 -0.16318387  0.04931256 -0.03621514]\n",
            "æ¨¡åž‹æˆªè·: 2.009165668232567\n",
            "\n",
            "âœ… Submission saved to:'submission_afterimprove.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zIUSd8IonILk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3dY1MZHRnIr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GfVdMdN0nI4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Xiaocheng Ma parts\n",
        "The cells below are for Xiaocheng Ma"
      ],
      "metadata": {
        "id": "vlpIF4minJZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "# 1. Improved Earthquake Damage SVM Model\n",
        "# ======================================\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# ======================================\n",
        "# 2. Load data\n",
        "# ======================================\n",
        "train_path = os.path.join(data_path, 'train.csv')\n",
        "test_path = os.path.join(data_path, 'test.csv')\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "test_df = pd.read_csv(test_path)\n",
        "\n",
        "test_building_id = test_df[\"building_id\"].copy()\n",
        "\n",
        "# ======================================\n",
        "# 3. Encode categorical columns\n",
        "# ======================================\n",
        "cat_cols = train_df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "for col in cat_cols:\n",
        "    train_df[col] = train_df[col].astype(\"category\").cat.codes\n",
        "    test_df[col] = test_df[col].astype(\"category\").cat.codes\n",
        "\n",
        "# ======================================\n",
        "# 4. Prepare Features\n",
        "# ======================================\n",
        "X = train_df.drop([\"building_id\", \"damage_grade\"], axis=1).values\n",
        "Y = train_df[\"damage_grade\"].values.astype(int) - 1\n",
        "X_test = test_df.drop([\"building_id\"], axis=1).values\n",
        "\n",
        "# === Remove low-variance features (helps SVM) ===\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "X = selector.fit_transform(X)\n",
        "X_test = selector.transform(X_test)\n",
        "\n",
        "# === Scale features ===\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# === Optional PCA (recommended if features > 50) ===\n",
        "pca = PCA(n_components=0.95, random_state=42)\n",
        "X = pca.fit_transform(X)\n",
        "X_test = pca.transform(X_test)\n",
        "\n",
        "# === Compute class weights ===\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(Y), y=Y)\n",
        "class_weights_dict = {i: w for i, w in enumerate(class_weights)}\n",
        "\n",
        "# ======================================\n",
        "# 5. Hyperparameter Search\n",
        "# ======================================\n",
        "param_grid = {\n",
        "    \"kernel\": [\"rbf\", \"poly\"],\n",
        "    \"C\": [0.5, 1, 3, 5],\n",
        "    \"gamma\": [\"scale\", 0.01, 0.05, 0.1],\n",
        "    \"degree\": [2, 3]  # for poly kernel\n",
        "}\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "svm = SVC(class_weight=class_weights_dict)\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    svm,\n",
        "    param_grid,\n",
        "    cv=kfold,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "print(\"ðŸ”Ž Running GridSearchCV for hyperparameter tuning...\\n\")\n",
        "grid.fit(X, Y)\n",
        "\n",
        "print(\"\\nðŸ”¥ BEST PARAMS FOUND:\", grid.best_params_)\n",
        "print(\"ðŸ”¥ BEST CV SCORE:\", grid.best_score_)\n",
        "\n",
        "# ======================================\n",
        "# 6. Train final model using best parameters\n",
        "# ======================================\n",
        "final_model = grid.best_estimator_\n",
        "final_model.fit(X, Y)\n",
        "\n",
        "# Evaluate training performance\n",
        "train_pred = final_model.predict(X)\n",
        "print(\"\\nTraining Accuracy:\", accuracy_score(Y, train_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(Y, train_pred))\n",
        "\n",
        "# ======================================\n",
        "# 7. Predict test set\n",
        "# ======================================\n",
        "y_test_pred = final_model.predict(X_test)\n",
        "y_test_pred = y_test_pred + 1  # convert back to 1/2/3\n",
        "\n",
        "# Save submission\n",
        "submission = pd.DataFrame({\n",
        "    \"building_id\": test_building_id,\n",
        "    \"damage_grade\": y_test_pred\n",
        "})\n",
        "\n",
        "submission.to_csv(os.path.join(data_path, \"submission_svm_optimized.csv\"), index=False)\n",
        "print(\"\\nâœ… Submission saved as 'submission_svm_optimized.csv' in Google Drive\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iespey58nUyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64708a7d-22b4-48ff-8fa8-6c950406157f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Fold 1 Accuracy: 0.4913\n",
            "Fold 2 Accuracy: 0.4963\n",
            "Fold 3 Accuracy: 0.4950\n",
            "Fold 4 Accuracy: 0.4825\n",
            "Fold 5 Accuracy: 0.4975\n",
            "\n",
            "Mean Cross-Validation Accuracy: 0.4925\n",
            "\n",
            "Accuracy on full training set: 0.58275\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.83      0.70       729\n",
            "           1       0.75      0.35      0.48      1968\n",
            "           2       0.50      0.80      0.61      1303\n",
            "\n",
            "    accuracy                           0.58      4000\n",
            "   macro avg       0.62      0.66      0.60      4000\n",
            "weighted avg       0.64      0.58      0.56      4000\n",
            "\n",
            "\n",
            "âœ… Submission saved to: 'submission.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "8n0TgJj8fVEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nn2iHPvunU-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Last code cells:"
      ],
      "metadata": {
        "id": "lOfbtpyHnV3H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y83btdSZnYgL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}