# Building Damage Grade Classification â€” Detailed README

**æœ€åæ›´æ–°ï¼š2025-12-02**

## ä¸€ã€é¡¹ç›®æ¦‚è§ˆ
æœ¬ä»“åº“ç”¨äºå»ºç­‘ç‰©æŸä¼¤ç­‰çº§ï¼ˆdamage_gradeï¼Œä¸‰åˆ†ç±»ï¼‰é¢„æµ‹ã€‚ç›®æ ‡ä»¥ weighted F1 ä¸ºæ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼Œå·¥ä½œåŒ…å«æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€å•æ¨¡å‹è®­ç»ƒï¼ˆHGB ä¸ NNï¼‰ã€è¶…å‚æœç´¢ã€èåˆä¸åå¤„ç†ã€ä»¥åŠç»“æœè¯Šæ–­ä¸æäº¤æµç¨‹ã€‚

æœ¬æ–‡æ¡£ç›®æ ‡ï¼š
- æ±‡æ€»å½“å‰æœ€ä½³å®è·µä¸æœ€ç»ˆæ–¹æ¡ˆï¼ˆHGB 5 æŠ˜é›†æˆ + Class3 åå¤„ç†ï¼‰
- ç»™å‡ºå¯å¤ç°çš„è®­ç»ƒ/æ¨ç†æ­¥éª¤
- è¯´æ˜æ¯ä¸ªè„šæœ¬çš„å…³é”®å¯é…ç½®å‚æ•°ä¸é»˜è®¤å€¼ï¼Œä¾¿äºå¿«é€Ÿè°ƒè¯•

## äºŒã€ç›®å½•ä¸å…³é”®æ–‡ä»¶ï¼ˆå¿«é€Ÿç´¢å¼•ï¼‰
- `src/`    : æ ¸å¿ƒè„šæœ¬ï¼ˆè®­ç»ƒã€è¯„ä¼°ã€æ¨ç†ã€èåˆï¼‰
- `models/` : æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆ`best_model_fold_*.pth`ï¼‰
- `tools/`  : å·¥å…·è„šæœ¬ï¼ˆè¯„ä¼°/æ£€æŸ¥ç‚¹/å¯è§†åŒ–ï¼‰
- `data/`   : æ•°æ®æ–‡ä»¶ï¼ˆ`train.csv`, `test.csv`, `submission.csv`ï¼‰

ä¸»è¦è„šæœ¬ï¼š
- `src/hgb_focused_solution.py` â€” HGB 5 æŠ˜ + åå¤„ç†ï¼Œä¸€é”®ç”Ÿæˆæäº¤
- `src/train_fast.py` â€” å¿«é€Ÿè®­ç»ƒï¼ˆNN ç¤ºä¾‹/ç®¡é“ï¼‰
- `src/net_optimized_edition.py` â€” NN è®­ç»ƒæµæ°´çº¿ï¼ˆmixup/EMA/FocalLossï¼‰
- `src/quick_ensemble.py` â€” NN+HGB åŠ æƒèåˆç¤ºä¾‹
- `src/deep_diagnosis.py` â€” æ€§èƒ½è¯Šæ–­ä¸ç½®ä¿¡åº¦åˆ†æ

## ä¸‰ã€å¿«é€Ÿå¼€å§‹ï¼ˆPowerShellï¼‰
1. åˆ‡æ¢åˆ°é¡¹ç›®ç›®å½•ï¼š
```powershell
cd "C:\Users\simpe\OneDrive\MCS\412 Data Mining\Project"
```
2. ç”Ÿæˆæœ€ç»ˆæäº¤ï¼ˆHGB ä¸»å¯¼æ–¹æ¡ˆï¼Œæœ€å¿«ï¼‰ï¼š
```powershell
python src\hgb_focused_solution.py
```
3. è¿è¡Œè¯Šæ–­åˆ†æï¼š
```powershell
python src\deep_diagnosis.py
```
4. ä»å¤´è®­ç»ƒï¼ˆNNï¼Œè€—æ—¶ï¼‰ï¼š
```powershell
python src\net_optimized_edition.py
```

## å››ã€æ•°æ®ä¸é¢„å¤„ç†è¦ç‚¹
- åˆå¹¶ `train.csv` ä¸ `test.csv` åšç»Ÿä¸€ç±»åˆ«ç¼–ç ï¼Œé¿å…è®­ç»ƒ/æµ‹è¯•ç¼–ç ä¸ä¸€è‡´
- æ•°å€¼ç‰¹å¾ï¼šå»ºè®®ä½¿ç”¨ `StandardScaler`ï¼ˆNNï¼‰ï¼Œæ ‘æ¨¡å‹å¯ä¸ç¼©æ”¾
- ç¼ºå¤±å€¼ï¼šæ•°å€¼ç”¨ä¸­ä½æ•°æˆ–æŒ‡ç¤ºå™¨å¡«å……ï¼›ç±»åˆ«ç”¨ `"missing"` æˆ–ä¼—æ•°
- åˆ†å±‚ CVï¼š`StratifiedKFold(n_splits=5, shuffle=True, random_state=42)`

## äº”ã€æ¨¡å‹ä¸è®­ç»ƒç»†èŠ‚ï¼ˆè¦ç‚¹ï¼‰

### HGBï¼ˆHistGradientBoostingï¼‰
- å»ºè®®é…ç½®ï¼š`max_iter=300-500`, `learning_rate=0.03-0.1`, `max_leaf_nodes` è§†æƒ…å†µè®¾ç½®
- ä¼˜ç‚¹ï¼šè®­ç»ƒå¿«ã€ç¨³å®šã€æ¦‚ç‡è´¨é‡å¥½

### ç¥ç»ç½‘ç»œï¼ˆFinalNet / ImprovedNetï¼‰
- æ¶æ„è¦ç‚¹ï¼šè¾“å…¥ â†’ 512 â†’ 256 â†’ 128 â†’ 64 â†’ è¾“å‡ºï¼ˆ3ï¼‰
- æ¯å±‚ BatchNorm + GELU + Dropoutï¼ˆ0.4â†’0.3â†’0.3â†’0.2ï¼‰
- è®­ç»ƒæŠ€å·§ï¼šmixupã€FocalLoss / class weightsã€EMAã€AdamWã€Cosine LR

## å…­ã€èåˆä¸åå¤„ç†
- èåˆï¼šåŠ æƒå¹³å‡ï¼ˆNN/HGBï¼‰ã€stackingï¼ˆOOF æ¦‚ç‡ä½œä¸ºå…ƒç‰¹å¾ï¼‰
- å…³é”®åå¤„ç†ï¼šå¯¹ Class3 æ¦‚ç‡åº”ç”¨æ”¾å¤§å› å­ï¼ˆä¾‹å¦‚ 1.5xâ€“2.5xï¼‰ï¼Œå†å½’ä¸€åŒ–
- æ¦‚ç‡æ ¡å‡†ï¼šæ¸©åº¦ç¼©æ”¾æˆ– Platt/isotonic æ ¡å‡†ï¼ˆå¦‚éœ€æ›´ç²¾ç¡®æ¦‚ç‡ï¼‰

## ä¸ƒã€è¯„ä¼°æŒ‡æ ‡ä¸è¯Šæ–­
- ä¸»è¦æŒ‡æ ‡ï¼šweighted F1ï¼ˆä¸»è¦ï¼‰ã€per-class precision/recall/f1
- è¯Šæ–­è„šæœ¬ï¼š`src/deep_diagnosis.py`ï¼ˆæ¯æŠ˜æ··æ·†çŸ©é˜µã€ç½®ä¿¡åº¦ã€æ¨¡å‹å¯¹æ¯”ï¼‰

## å…«ã€è„šæœ¬å‚æ•°è¯´æ˜è¡¨ï¼ˆå¿«é€Ÿå‚è€ƒï¼‰
ä¸‹é¢åˆ—å‡º `src/` ä¸‹å…³é”®è„šæœ¬çš„ä¸»è¦å¯é…ç½®å‚æ•°ã€é»˜è®¤å€¼ä¸ä»£ç ä½ç½®ï¼Œä¾¿äºä½ å¿«é€Ÿå®šä½ä¸ä¿®æ”¹ã€‚

### `src/train_fast.py`
| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | ä»£ç ä½ç½® |
|---|---:|---|---|
| random seed | 42 | éšæœºç§å­ï¼Œå½±å“å¤ç° | `set_seed(seed=42)` å‡½æ•°è°ƒç”¨å¤„ |
| device | è‡ªåŠ¨æ£€æµ‹ | CUDA/CPU è‡ªåŠ¨é€‰æ‹© | `device = torch.device(...)` |
| batch size | 32 | è®­ç»ƒå°æ‰¹é‡å¤§å°ï¼ˆåœ¨å¾ªç¯ä¸­ç¡¬ç¼–ç ï¼‰ | `for i in range(0, len(X_tr), 32)` |
| lr | 1e-2 | AdamW åˆå§‹å­¦ä¹ ç‡ | `optimizer = torch.optim.AdamW(..., lr=1e-2)` |
| weight_decay | 1e-4 | AdamW æƒé‡è¡°å‡ | åŒä¸Š |
| scheduler | CosineAnnealingWarmRestarts(T_0=15, T_mult=2) | LR è°ƒåº¦ | `scheduler = ...` |
| epochs | 100 | æœ€å¤§è®­ç»ƒè½®æ•° | `for epoch in range(100):` |
| validate freq | æ¯ 10 epoch | éªŒè¯é¢‘ç‡ï¼ˆå½“å‰å®ç°ï¼‰ | `if (epoch+1)%10 == 0:` |
| patience | 20 | æ—©åœå®¹å¿ | `patience = 20` |
| class_weights tweak | cw[1] *= 2.0 | æ‰‹åŠ¨æ”¾å¤§ class 2 æƒé‡ï¼ˆå¯åˆ æ”¹ï¼‰ | class_weights è®¡ç®—åç›´æ¥ä¿®æ”¹ |

### `src/hgb_focused_solution.py`
| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | ä»£ç ä½ç½® |
|---|---:|---|---|
| n_splits | 5 | StratifiedKFold æŠ˜æ•° | `StratifiedKFold(n_splits=5, ...)` |
| HGB max_iter | 500 | HGB è¿­ä»£æ¬¡æ•° | `HistGradientBoostingClassifier(max_iter=500, ...)` |
| HGB learning_rate | 0.08 | å­¦ä¹ ç‡ | åŒä¸Š |
| HGB max_leaf_nodes | 50 | æ§åˆ¶æ¨¡å‹å®¹é‡ | åŒä¸Š |
| l2_regularization | 0.01 | L2 æ­£åˆ™åŒ– | åŒä¸Š |
| max_bins | 300 | åˆ†ç®±æ•° | åŒä¸Š |
| class boosts | C1=0.9, C2=1.0, C3=2.5 | ä¸‰ç±»æ¦‚ç‡æ”¾å¤§å› å­ï¼ˆè°ƒæ•´ä»¥æ§åˆ¶å¬å›ï¼‰ | åœ¨ `boosted_probs[:, i] *= factor` å¤„ |
| submission path | `data/submission.csv` | è¾“å‡ºæ–‡ä»¶è·¯å¾„ | `submission.to_csv(...)` |

### `src/net_optimized_edition.py`
| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | ä»£ç ä½ç½® |
|---|---:|---|---|
| seed | 42 | éšæœºç§å­ | `set_seed(42)` |
| batch_size (train) | 64 | è®­ç»ƒæ—¶ batch å¤§å° | `DataLoader(..., batch_size=64)` |
| batch_size (val) | 256 | éªŒè¯æ—¶ batch å¤§å° | `DataLoader(..., batch_size=256)` |
| optimizer lr | 1e-2 | AdamW åˆå§‹ lr | `torch.optim.AdamW(..., lr=1e-2)` |
| weight_decay | 1e-4 | æƒé‡è¡°å‡ | åŒä¸Š |
| epochs | 120 | æœ€å¤§è®­ç»ƒè½®æ•° | `epochs = 120` |
| scheduler | CosineAnnealingWarmRestarts(T_0=20, T_mult=2, eta_min=1e-5) | LR ç­–ç•¥ | `scheduler = ...` |
| criterion | FocalLoss(alpha=class_weights, gamma=2.0) | å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ | `criterion = FocalLoss(...)` |
| mixup alpha | 0.3 | Mixup å¼ºåº¦ | `mixup_batch(..., alpha=0.3)` |
| EMA decay | 0.999 | EMA è¡°å‡ | `EMA(model, decay=0.999)` |
| clip_grad_norm | 1.0 | æ¢¯åº¦è£å‰ªé˜ˆå€¼ | `clip_grad_norm_(..., max_norm=1.0)` |
| sampler | WeightedRandomSampler | é€šè¿‡æ ·æœ¬æƒé‡ä¸Šé‡‡æ · | `create_sampler(y_train)` |
| patience | 25 | æ—©åœå®¹å¿ | `patience = 25` |

### `src/quick_ensemble.py`
| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | ä»£ç ä½ç½® |
|---|---:|---|---|
| NN weight | 0.65 | èåˆæ—¶ NN æ¦‚ç‡æƒé‡ | `final_probs = 0.65 * nn_probs + 0.35 * hgb_probs` |
| HGB weight | 0.35 | èåˆæ—¶ HGB æƒé‡ | åŒä¸Š |
| HGB params | max_iter=300, lr=0.1, l2=0.1 | åœ¨ ensemble ä¸­è®­ç»ƒ HGB çš„è®¾ç½® | `HistGradientBoostingClassifier(...)` |
| model filenames | `best_model_fold_{fold}.pth` | æœŸæœ›çš„ NN checkpoint åç§° | `load_existing_models` æ£€æŸ¥å¤„ |
| test batch size | 512 | æ¨ç†æ—¶ batch | `DataLoader(..., batch_size=512)` |

### `src/deep_diagnosis.py`
| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | ä»£ç ä½ç½® |
|---|---:|---|---|
| n_splits | 5 | CV æŠ˜æ•°ç”¨äºè¯Šæ–­ | `StratifiedKFold(n_splits=5, ...)` |
| HGB params | max_iter=350, lr=0.09 | è¯Šæ–­æ—¶ä¸ NN å¯¹æ¯”ç”¨çš„ HGB è®¾ç½® | `HistGradientBoostingClassifier(...)` |
| è¾“å‡ºæ–¹å¼ | æ§åˆ¶å°æ‰“å° & DataFrame | è¾“å‡ºæ··æ·†çŸ©é˜µã€per-class æŒ‡æ ‡ã€fold æ±‡æ€» | æ‰“å°ä¸ `metrics_df` éƒ¨åˆ† |

---

## ä¹ã€å¯é€‰æ”¹è¿›ï¼ˆæˆ‘å¯ä»¥æ›¿ä½ å®ç°ï¼‰
- å°†ä¸Šè¿°è„šæœ¬ç»Ÿä¸€æ”¹ä¸ºæ”¯æŒ `argparse` å‘½ä»¤è¡Œå‚æ•°ï¼Œä»¥ä¾¿åœ¨ CI/å®éªŒä¸­ç›´æ¥ä¼ å‚
- å°†è¶…å‚é›†ä¸­åˆ° `config.yaml` å¹¶åœ¨è„šæœ¬ä¸­åŠ è½½ï¼Œä¾¿äºç‰ˆæœ¬åŒ–ç®¡ç†
- åœ¨ README ä¸­åŠ å…¥æ¯ä¸ªè„šæœ¬çš„ç¤ºä¾‹å‘½ä»¤ï¼ˆå«å¸¸ç”¨å‚æ•°ç»„åˆï¼‰

å¦‚æœä½ åŒæ„ï¼Œæˆ‘å¯ä»¥æŠŠ `train_fast.py` å’Œ `net_optimized_edition.py` æ”¹ä¸ºæ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼ˆç¤ºä¾‹ï¼š`--lr`, `--batch-size`, `--epochs`, `--out-dir`ï¼‰ï¼Œå¹¶æŠŠç¤ºä¾‹å‘½ä»¤åŠ å…¥ READMEã€‚

---

**ç»“æŸ** â€” å·²å°† README æ‰©å±•ä¸ºæ›´å…¨é¢çš„è¯´æ˜å¹¶åŠ å…¥è„šæœ¬å‚æ•°è¡¨ã€‚è‹¥éœ€è¦æˆ‘ç°åœ¨æŠŠä¸¤åˆ°ä¸‰ä¸ªå…³é”®è„šæœ¬æ”¹ä¸ºæ¥å—å‘½ä»¤è¡Œå‚æ•°ï¼Œè¯·å›å¤ â€œæ”¹ argâ€ æˆ–æŒ‡å®šå…·ä½“è„šæœ¬ã€‚

## é™„ï¼šæ¯ä¸ªä¼˜åŒ–çš„è¶…å‚æ•°ä¸ç½‘ç»œç»“æ„ï¼ˆè¯¦ç»†ï¼‰
ä¸‹é¢ç»™å‡ºåœ¨æœ¬é¡¹ç›®ä¸­ä½¿ç”¨/æµ‹è¯•çš„ä¸»è¦æ¨¡å‹é…ç½®ä¸ç½‘ç»œç»“æ„ç»†èŠ‚ï¼ŒåŒ…å«å…·ä½“è¶…å‚æ•°æ•°å€¼ä¸è®­ç»ƒè®¾ç½®ï¼Œæ–¹ä¾¿å¤ç°æˆ–å¾®è°ƒã€‚

### A. HGB æ¨¡å‹é…ç½®ï¼ˆå„è„šæœ¬ï¼‰
- `src/hgb_focused_solution.py`ï¼ˆæœ€ç»ˆæ–¹æ¡ˆä¸­çš„ HGBï¼‰
  - `max_iter`: 500
  - `learning_rate`: 0.08
  - `max_leaf_nodes`: 50
  - `l2_regularization`: 0.01
  - `max_bins`: 300
  - CV: StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
  - åå¤„ç†: class boosts = [0.9, 1.0, 2.5]ï¼ˆC1,C2,C3ï¼‰ï¼Œéšåé‡æ–°å½’ä¸€åŒ–

- `src/quick_ensemble.py`ï¼ˆensemble ä¸­è®­ç»ƒçš„ HGBï¼‰
  - `max_iter`: 300
  - `learning_rate`: 0.10
  - `l2_regularization`: 0.10
  - ç”¨é€”ï¼šåœ¨ ensemble ä¸­å¿«é€Ÿè®­ç»ƒä¸€ä»½ HGB ä½œä¸ºä¸ NN åŠ æƒèåˆçš„åŸºçº¿

- `src/deep_diagnosis.py`ï¼ˆå¯¹æ¯”ç”¨ HGBï¼‰
  - `max_iter`: 350
  - `learning_rate`: 0.09

> è¯´æ˜ï¼šè‹¥éœ€æ”¹å˜ HGB è¶…å‚æ•°ï¼Œä¿®æ”¹å¯¹åº”è„šæœ¬ä¸­ `HistGradientBoostingClassifier(...)` çš„å‚æ•°å³å¯ï¼Œæˆ–å°†å‚æ•°æš´éœ²ä¸ºå‘½ä»¤è¡Œ/é…ç½®é¡¹ä»¥ä¾¿æ‰¹é‡æœç´¢ã€‚

---

### B. ç¥ç»ç½‘ç»œç»“æ„ä¸è®­ç»ƒè¶…å‚æ•°
ä¸‹é¢åˆ—å‡ºé¡¹ç›®ä¸­å‡ºç°çš„ä¸»è¦ç½‘ç»œå®ç°ï¼ˆæ–‡ä»¶ã€ç±»åä¸ç»“æ„ï¼‰ï¼Œå¹¶ç»™å‡ºè®­ç»ƒæ—¶çš„å…³é”®è¶…å‚æ•°ã€‚

1) `OptimizedNet`ï¼ˆ`src/train_fast.py` ä¸ `src/hgb_focused_solution.py` çš„å®ç°å˜ä½“ï¼‰
   - ç»“æ„ï¼ˆé¡ºåºï¼‰ï¼šBatchNorm(input) â†’ Linear(input,512) â†’ BN(512) â†’ GELU â†’ Dropout(0.5) â†’ Linear(512,256) â†’ BN(256) â†’ GELU â†’ Dropout(0.4) â†’ Linear(256,128) â†’ BN(128) â†’ GELU â†’ Dropout(0.3) â†’ Linear(128,64) â†’ GELU â†’ Dropout(0.2) â†’ Linear(64,3)
   - æŸå¤±ï¼šCrossEntropyLossï¼ˆtrain_fast ä¸­ä½¿ç”¨ class weightsï¼Œå¹¶å¯¹ class 2 åšäº†äººå·¥æ”¾å¤§ cw[1]*=2.0ï¼Œå¯åˆ ï¼‰
   - ä¼˜åŒ–å™¨ï¼šAdamW(lr=1e-2, weight_decay=1e-4)
   - Schedulerï¼šCosineAnnealingWarmRestarts(T_0=15, T_mult=2)
   - æ‰¹é‡/è½®æ•°ï¼šbatch_size=32ï¼ˆtrain_fastï¼‰ï¼Œepochs up to 100ï¼Œvalidate æ¯ 10 epoch
   - æ—©åœï¼špatience=20

2) `ImprovedNet`ï¼ˆ`src/net_optimized_edition.py` ä¸­çš„å®ç°ï¼‰
   - ç»“æ„ï¼ˆæ¨¡å—åŒ–ï¼‰ï¼š
     - layer1: Linear(input,512) â†’ BatchNorm1d(512) â†’ GELU â†’ Dropout(0.4)
     - layer2: Linear(512,256) â†’ BatchNorm1d(256) â†’ GELU â†’ Dropout(0.3)
     - layer3: Linear(256,128) â†’ BatchNorm1d(128) â†’ GELU â†’ Dropout(0.3)
     - layer4: Linear(128,64) â†’ BatchNorm1d(64) â†’ GELU â†’ Dropout(0.2)
     - head: Linear(64,3)
   - è®­ç»ƒç»†èŠ‚ï¼ˆé»˜è®¤ï¼‰ï¼š
     - æŸå¤±ï¼šFocalLoss(alpha=class_weights, gamma=2.0)
     - ä¼˜åŒ–å™¨ï¼šAdamW(lr=1e-2, weight_decay=1e-4)
     - Schedulerï¼šCosineAnnealingWarmRestarts(T_0=20, T_mult=2, eta_min=1e-5)
     - Mixupï¼šalpha=0.3ï¼ˆæ¯ä¸ª batch ä½¿ç”¨ mixup_batchï¼‰
     - EMAï¼šdecay=0.999ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ç»´æŠ¤ EMA æƒé‡
     - æ‰¹é‡ï¼štrain batch_size=64ï¼Œval batch_size=256
     - epochsï¼šæœ€å¤š 120ï¼Œæ—©åœ patience=25
     - é‡‡æ ·å™¨ï¼šWeightedRandomSamplerï¼ˆæŒ‰ç±»åˆ«é¢‘ç‡ä¸Šé‡‡æ ·ä»¥å¹³è¡¡ï¼‰

3) `WideNet`ï¼ˆ`src/quick_ensemble.py` ä¸­ç”¨äºåŠ è½½ checkpoint çš„ç½‘ç»œï¼‰
   - ç±»ä¼¼äº OptimizedNetï¼Œå¸¦æœ‰å‘½å dropout å±‚ï¼ˆdropouts: 0.5, 0.4, 0.3, 0.2ï¼‰
   - ç”¨äºåŠ è½½ `best_model_fold_{i}.pth` å¹¶æ¨ç†å‡º NN æ¦‚ç‡

4) `FinalNet`ï¼ˆé¡¹ç›®ä¸­ç¤ºä¾‹/æœ€ç»ˆå®ç°ï¼Œå‚è§ `src/net_final_v2.py`ï¼Œå¦‚æœªå­˜åœ¨è¯·å‚è€ƒæ­¤å®šä¹‰ï¼‰
   - æ¶æ„è¦ç‚¹ï¼šLinear(input,512) â†’ BN â†’ GELU â†’ Linear(512,256)+res_proj(512â†’256) â†’ BN â†’ GELU â†’ Dropout â†’ Linear(256,128) â†’ BN â†’ GELU â†’ Dropout â†’ Linear(128,64) â†’ BN â†’ GELU â†’ Dropout â†’ head Linear(64, num_classes)
   - æƒé‡åˆå§‹åŒ–ï¼šXavierï¼ˆxavier_uniform_ï¼‰
   - è®­ç»ƒå»ºè®®ï¼šAdamW lr=1e-3ï¼ˆæˆ– 1e-2 è§†å®éªŒï¼‰ï¼Œmixup alphaâ‰ˆ0.2ï¼Œlabel_smoothingâ‰ˆ0.05ï¼ŒFocalLoss å¯é€‰

---

### C. èåˆä¸è®­ç»ƒè¶…å‚æ‘˜è¦ï¼ˆå¸¸ç”¨é»˜è®¤å€¼ï¼‰
- èåˆæƒé‡ï¼ˆ`quick_ensemble.py`ï¼‰ï¼šNN 0.65 / HGB 0.35
- HGB åå¤„ç†ï¼ˆ`hgb_focused_solution.py`ï¼‰ï¼šclass boosts = [0.9, 1.0, 2.5]
- å¸¸ç”¨è®­ç»ƒæ‰¹æ¬¡ä¸è½®æ•°ï¼šNN train batch 64ã€epochs 80â€“120ï¼›HGB å•æ¬¡è®­ç»ƒå³å®Œæˆ
- ä¼˜åŒ–å™¨/æ­£åˆ™ï¼šAdamWï¼ˆlr 1e-2 æˆ– 1e-3ï¼‰+ weight_decay 1e-4ï¼›gradient clip norm=1.0ï¼ˆåœ¨ net_optimized_edition ä¸­ä½¿ç”¨ï¼‰

---

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ï¼š
- æŠŠä»¥ä¸Šé»˜è®¤è¶…å‚æ•°æå–åˆ° `config.yaml` å¹¶è®©è„šæœ¬ä»ä¸­è¯»å–ï¼›æˆ–
- å°† `net_optimized_edition.py` å’Œ `train_fast.py` æ”¹ä¸ºæ”¯æŒ `argparse`ï¼ˆæ¨èï¼Œä»¥ä¾¿å‘½ä»¤è¡Œä¼ å‚ä¸å®éªŒè‡ªåŠ¨åŒ–ï¼‰ã€‚

è¯·å‘Šè¯‰æˆ‘ä½ æƒ³å…ˆåšå“ªé¡¹ï¼ˆä¾‹å¦‚ï¼šâ€œæ”¹ config.yamlâ€ æˆ– â€œä¸º net_optimized_edition æ”¹ argparseâ€ï¼‰ã€‚

# Building Damage Grade Classification â€” Detailed README

æœ€åæ›´æ–°ï¼š2025-12-02

## ä¸€ã€é¡¹ç›®æ¦‚è§ˆ
æœ¬ä»“åº“ç”¨äºå»ºç­‘ç‰©æŸä¼¤ç­‰çº§ï¼ˆdamage_gradeï¼Œä¸‰åˆ†ç±»ï¼‰é¢„æµ‹ã€‚ç›®æ ‡ä»¥ weighted F1 ä¸ºæ ¸å¿ƒè¯„ä»·æŒ‡æ ‡ï¼Œå·¥ä½œåŒ…å«æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€å•æ¨¡å‹è®­ç»ƒï¼ˆHGB ä¸ NNï¼‰ã€è¶…å‚æœç´¢ã€èåˆä¸åå¤„ç†ã€ä»¥åŠç»“æœè¯Šæ–­ä¸æäº¤æµç¨‹ã€‚

## äºŒã€ç›®å½•ä¸å…³é”®æ–‡ä»¶
- `src/`    : æ ¸å¿ƒè„šæœ¬ï¼ˆè®­ç»ƒã€è¯„ä¼°ã€æ¨ç†ã€èåˆï¼‰
- `models/` : æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆ`best_model_fold_*.pth`ï¼‰
- `tools/`  : å·¥å…·è„šæœ¬ï¼ˆè¯„ä¼°/æ£€æŸ¥ç‚¹/å¯è§†åŒ–ï¼‰
- `data/`   : æ•°æ®æ–‡ä»¶ï¼ˆ`train.csv`, `test.csv`, `submission.csv`ï¼‰
- `README.md`: æœ¬æ–‡æ¡£ï¼ˆè¯¦ç»†è¯´æ˜ï¼‰

ä¸»è¦è„šæœ¬å¿«é€Ÿç´¢å¼•ï¼š
- `src/hgb_focused_solution.py` â€” HGB 5 æŠ˜ + åå¤„ç†ï¼Œä¸€é”®ç”Ÿæˆ `data/submission.csv`ï¼ˆæ¨èç”¨äºæäº¤ï¼‰
- `src/train_fast.py` â€” HGB è®­ç»ƒæµæ°´çº¿ï¼ˆç”¨äº CVï¼‰
- `src/deep_diagnosis.py` â€” è¯Šæ–­è„šæœ¬ï¼ˆæ··æ·†çŸ©é˜µã€per-class æŒ‡æ ‡ã€è¯¯åˆ†ç±»æ ·æœ¬ï¼‰
- `src/net_final_v2.py` â€” NN æ¨¡å‹å®šä¹‰ï¼ˆFinalNetï¼‰
- `src/net_optimized_edition.py` â€” NN è®­ç»ƒæµç¨‹ï¼ˆmixupã€EMAã€FocalLoss å¯é€‰ï¼‰
- `src/quick_ensemble.py`, `src/stack_ensemble.py` â€” èåˆä¸å…ƒå­¦ä¹ è„šæœ¬

## ä¸‰ã€è¿è¡Œç¯å¢ƒä¸ä¾èµ–
- å»ºè®® Python ç‰ˆæœ¬ï¼š3.9â€“3.11
- å»ºè®®ä¾èµ–ï¼ˆç¤ºä¾‹ï¼‰ï¼š
```powershell
pip install -r requirements.txt
# è‹¥æ²¡æœ‰ requirements.txtï¼Œè‡³å°‘å®‰è£…ï¼š
pip install pandas numpy scikit-learn torch torchvision optuna matplotlib seaborn
```

## å››ã€æ•°æ®é¢„å¤„ç†ï¼ˆè¯¦ç»†æµç¨‹ï¼‰
1. ç»Ÿä¸€ç¼–ç ï¼šå°† `train.csv` ä¸ `test.csv` åˆå¹¶åå¯¹ç±»åˆ«ç‰¹å¾ç»Ÿä¸€ Label Encoding / Ordinal Encodingï¼Œä¿è¯è®­ç»ƒ/æµ‹è¯•ä¸€è‡´æ€§ã€‚é¿å…å•ç‹¬å¯¹ train ç¼–ç å¸¦æ¥çš„æ³„éœ²ã€‚ 
2. ç¼ºå¤±å€¼å¤„ç†ï¼š
   - å¯¹æ•°å€¼å‹ï¼šæŒ‰åˆ—ä¸­ä½æ•°æˆ–ä½¿ç”¨å¸¦æŒ‡ç¤ºå™¨çš„å¡«å……ï¼ˆè‹¥ç¼ºå¤±å…·æœ‰å«ä¹‰ï¼‰
   - å¯¹ç±»åˆ«å‹ï¼šå¡«å……ç‰¹æ®Šç±»åˆ« `"missing"` æˆ–é¢‘ç‡æœ€é«˜é¡¹
3. å¼‚å¸¸å€¼å¤„ç†ï¼šå¯¹æ˜æ˜¾å¼‚å¸¸çš„æ•°å€¼ç‰¹å¾åš winsorization æˆ–æŒ‰ä¸Šä¸‹é™æˆªæ–­
4. ç‰¹å¾ç¼©æ”¾ï¼šå¯¹ NN ä½¿ç”¨ `StandardScaler`ï¼ˆfit åœ¨è®­ç»ƒé›†ä¸Šï¼‰ï¼Œå¯¹æ ‘æ¨¡å‹å¯ä¸åšç¼©æ”¾
5. åˆ†å±‚åˆ’åˆ†ï¼šä½¿ç”¨ `StratifiedKFold(n_splits=5, shuffle=True, random_state=42)` åš CV åˆ†å‰²

## äº”ã€ç‰¹å¾å·¥ç¨‹ï¼ˆå»ºè®®ä¸å·²å®ç°é¡¹ï¼‰
- åŸå§‹ç‰¹å¾ä¿ç•™ä¸æ¸…æ´—ï¼ˆå¦‚å»ºç­‘é¢ç§¯ã€æ¥¼å±‚æ•°ã€æè´¨ç¼–ç ç­‰ï¼‰
- æ„é€ äº¤äº’ç‰¹å¾ï¼šæ•°å€¼ç›¸é™¤/ä¹˜/æ¯”å€¼ã€ç±»åˆ«ç»„åˆï¼ˆé¢‘ç‡ç¼–ç  / target encodingï¼‰
- èšåˆç‰¹å¾ï¼šè‹¥å­˜åœ¨åˆ†ç»„ï¼ˆä¾‹å¦‚åœ°åŒºã€ç¤¾åŒºï¼‰ï¼Œåš groupby èšåˆï¼ˆå‡å€¼ã€æ–¹å·®ã€è®¡æ•°ï¼‰
- ç»Ÿè®¡ç‰¹å¾ï¼šç¼ºå¤±ç‡ã€éé›¶è®¡æ•°ã€ç‰¹å¾åˆ†å¸ƒååº¦/å³°åº¦ç­‰
- ç‰¹å¾é€‰æ‹©ï¼šåŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ç­›é€‰ã€äº’ä¿¡æ¯æˆ–é€æ­¥åˆ é™¤æ³•

å®ç°æç¤ºï¼šæŠŠ feature pipeline ç‹¬ç«‹ä¸º `src/features.py`ï¼ˆå»ºè®®åšæ³•ï¼‰ï¼Œæ–¹ä¾¿åœ¨ NN ä¸ HGB ä¹‹é—´å…±äº«ç‰¹å¾å¤„ç†é€»è¾‘ã€‚

## å…­ã€æ¨¡å‹å®ç°ä¸è®­ç»ƒç»†èŠ‚

### 6.1 HistGradientBoosting (HGB) â€” ä¸»åŠ›æ¨¡å‹
- é…ç½®å»ºè®®ï¼š`max_iter=300-1000`, `learning_rate=0.03-0.1`, `max_leaf_nodes` æˆ– `max_depth` è§†ç‰¹å¾ç»´åº¦è°ƒæ•´
- è®­ç»ƒè¦ç‚¹ï¼šä½¿ç”¨ early_stoppingï¼ˆè‡ªåŠ¨ï¼‰å¹¶ä¿å­˜æ¯æŠ˜æœ€ä¼˜æ¨¡å‹ï¼ˆæŒ‰ val weighted F1ï¼‰
- è¾“å‡ºï¼šä¿å­˜æ¯æŠ˜æ¦‚ç‡å¹¶å¯¹æµ‹è¯•å–å¹³å‡ï¼ˆoof æ¦‚ç‡ç”¨äºèåˆ/stackingï¼‰

### 6.2 Neural Network (FinalNet) â€” è¡¨æ ¼ MLPï¼ˆ`src/net_final_v2.py`ï¼‰
- æ¶æ„ï¼šinput â†’ 512 â†’ 256 â†’ 128 â†’ 64 â†’ outputï¼ˆ3 classesï¼‰ï¼ŒGELU æ¿€æ´»ï¼Œæ¯å±‚ BatchNorm + Dropout
- æ®‹å·®ï¼š512â†’256 é—´åŠ å…¥çº¿æ€§æŠ•å½±æ®‹å·®ä»¥ç¨³å®šè®­ç»ƒ
- æƒé‡åˆå§‹åŒ–ï¼šXavierï¼ˆ`xavier_uniform_`ï¼‰

è®­ç»ƒç»†èŠ‚ï¼ˆNNï¼‰ï¼š
- æŸå¤±ï¼š`CrossEntropyLoss(weight=class_weights)` æˆ– `FocalLoss(gamma=2.0)`ï¼›å¯åŠ  label_smoothing
- ä¼˜åŒ–å™¨ï¼š`AdamW`ï¼Œåˆå§‹ lr=1e-3ï¼ˆå»ºè®®ä¸ OneCycle æˆ– CosineAnnealing è°ƒåº¦ç»“åˆï¼‰
- æ­£åˆ™ï¼šDropoutã€weight decayï¼ˆ1e-5â€“1e-3ï¼‰ã€mixup(alphaâ‰ˆ0.2)ã€EMA æƒé‡
- æ‰¹æ¬¡ä¸è½®æ•°ï¼šbatch_size=64â€“256ï¼ˆå—å†…å­˜é™åˆ¶ï¼‰ï¼Œepochs=50â€“120ï¼Œearly-stopï¼ˆpatienceâ‰ˆ15ï¼‰
- ä¿å­˜ï¼šæ¯æŠ˜ä¿å­˜æœ€ä¼˜ checkpointï¼ˆæŒ‰ val weighted F1ï¼‰ï¼Œå¹¶å¯¼å‡º val logits ä¸ test logits

### 6.3 è®­ç»ƒæµç¨‹ï¼ˆæ¨èï¼‰
1. å…ˆç”¨ `train_fast.py` è®­ç»ƒ HGB å¿«é€ŸåŸºçº¿å¹¶è·å– oof_probabilities
2. åœ¨ subset ä¸Šè°ƒè¯• NNï¼ˆç¡®ä¿ pipeline æ— é”™è¯¯ï¼‰
3. åœ¨å…¨éƒ¨æ•°æ®ä¸Šåš 5 æŠ˜ NN è®­ç»ƒï¼Œä¿å­˜æ¯æŠ˜ checkpoint ä¸ oof
4. åœ¨éªŒè¯ä¸Šåšèåˆæƒé‡ç½‘æ ¼æœç´¢ï¼ˆNN/HGB æƒé‡ + Class3 boostï¼‰

## ä¸ƒã€è¶…å‚æ•°æœç´¢ä¸è°ƒåº¦
- å·¥å…·ï¼šå»ºè®®ä½¿ç”¨ `Optuna` åšé«˜æ•ˆæœç´¢ï¼ˆå¯å¹¶è¡Œï¼‰
- æœç´¢ç›®æ ‡ï¼šval weighted F1ï¼ˆæœ€é‡è¦ï¼‰ï¼Œå…¶æ¬¡ç›‘æ§ per-class recallï¼ˆå°¤å…¶ Class3ï¼‰
- æœç´¢ç¤ºä¾‹ï¼šæœç´¢ HGB çš„ `learning_rate, max_iter, max_leaf_nodes`ï¼ŒNN çš„ `lr, weight_decay, dropout_rates, hidden_dims` 

ç¤ºä¾‹ Optuna é…ç½®ï¼ˆä¼ªä»£ç ï¼‰:
```python
def objective(trial):
  lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)
  wd = trial.suggest_loguniform('wd', 1e-6, 1e-2)
  # è®­ç»ƒå¹¶è¿”å› val weighted F1
```

## å…«ã€èåˆï¼ˆEnsemblingï¼‰ä¸åå¤„ç†
- èåˆç­–ç•¥ï¼šç®€å•åŠ æƒå¹³å‡ã€stackingï¼ˆä½¿ç”¨ oof æ¦‚ç‡è®­ç»ƒå…ƒæ¨¡å‹å¦‚ LogisticRegressionï¼‰ã€rank fusion
- åå¤„ç†ï¼ˆå…³é”®ï¼‰ï¼šClass3 æ¦‚ç‡æ”¾å¤§ï¼ˆä¾‹å¦‚ Ã—1.5â€“2.0ï¼‰å¹¶é‡æ–°å½’ä¸€åŒ–ä»¥æå‡å°‘æ•°ç±»å¬å›ï¼›åœ¨éªŒè¯é›†ä¸Šç½‘æ ¼æœç´¢æ”¾å¤§å› å­å¹¶é€‰æ‹©æœ€ç¨³å¥å€¼
- æ ¡å‡†ï¼šå¯ä½¿ç”¨æ¸©åº¦ç¼©æ”¾ï¼ˆtemperature scalingï¼‰æˆ– isotonic/Platt æ ¡å‡†ä¿®æ­£æ¨¡å‹æ¦‚ç‡

## ä¹ã€è¯„ä¼°ä¸è¯Šæ–­
- ä¸»è¦æŒ‡æ ‡ï¼šweighted F1ã€per-class F1ã€precisionã€recall
- è¯Šæ–­è„šæœ¬ï¼š`src/deep_diagnosis.py`ï¼ˆè¾“å‡ºæ¯æŠ˜æ··æ·†çŸ©é˜µã€è¯¯åˆ†ç±»æ ·æœ¬ï¼‰
- å¯è§†åŒ–ï¼šè®­ç»ƒ/éªŒè¯æ›²çº¿ã€ç‰¹å¾é‡è¦æ€§å›¾ã€æ··æ·†çŸ©é˜µçƒ­å›¾

## åã€å¤ç°ï¼ˆæ­¥éª¤ï¼‰
1. å‡†å¤‡ç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
2. å°† `data/` æ”¾åœ¨é¡¹ç›®æ ¹ï¼ˆåŒ…å« `train.csv`, `test.csv`ï¼‰
3. è¿è¡Œ HGB åŸºçº¿ï¼š
```powershell
python src\train_fast.py
```
4. å¯é€‰ï¼šè®­ç»ƒ NNï¼ˆå…ˆå°è§„æ¨¡è°ƒè¯•ï¼Œå†å…¨é‡è®­ç»ƒï¼‰ï¼š
```powershell
python src\net_optimized_edition.py
```
5. è¿è¡Œèåˆä¸ç”Ÿæˆæäº¤ï¼š
```powershell
python src\hgb_focused_solution.py
```

## åä¸€ã€è°ƒè¯•ä¸å¸¸è§é—®é¢˜
- æäº¤æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼šç¡®ä¿ `data/submission.csv` æ— ç´¢å¼•åˆ—ï¼Œåˆ—åä¸º `building_id,damage_grade`
- F1 åœ¨æœ¬åœ°é«˜ä½†æäº¤ä½ï¼šæ£€æŸ¥æ•°æ®æ³„éœ²ã€ç‰¹å¾ç¼–ç ä¸ä¸€è‡´ã€æˆ–æµ‹è¯•åˆ†å¸ƒåç§»ï¼›é€é¡¹å›æº¯ OOF ä¸ test æ¦‚ç‡åˆ†å¸ƒ
- NN è®­ç»ƒä¸æ”¶æ•›ï¼šå…ˆåœ¨å°æ ·æœ¬ä¸Šä½¿ç”¨è¾ƒå¤§å­¦ä¹ ç‡ä¸æ›´å¼ºæ­£åˆ™è°ƒè¯•ï¼Œç¡®è®¤æ•°æ® pipeline æ­£ç¡®

## åäºŒã€æ‰©å±•å®éªŒå»ºè®®ï¼ˆæœªæ¥å·¥ä½œï¼‰
- å°è¯•æ›´å¤šè¡¨æ ¼æ·±åº¦æ¨¡å‹ï¼ˆTabNetã€FT-Transformerï¼‰
- åŸºäº stacking çš„å¤šçº§èåˆï¼ˆå¤šæ¨¡å‹ + æ—¶é—´/åœ°ç‚¹åˆ†ç»„å…ƒå­¦ä¹ ï¼‰
- ä½¿ç”¨æ›´ç»†ç²’åº¦çš„ç±»åˆ«æ ¡å‡†æˆ–ä»£ä»·æ•æ„Ÿå­¦ä¹ ä»¥æå‡ Class3 å¬å›

---
å¦‚éœ€æˆ‘æŠŠ README è½¬ä¸º PDFã€æˆ–æŠŠéƒ¨åˆ†å‘½ä»¤æ”¹æˆ Linux/bash ç‰ˆæœ¬ï¼Œæˆ–æŒ‰ä½ å¸Œæœ›çš„æ ¼å¼å†ç»†åŒ–ï¼ˆä¾‹å¦‚æŠŠæ¯ä¸ªè„šæœ¬çš„å‚æ•°å†™æˆè¡¨æ ¼ï¼‰ï¼Œå‘Šè¯‰æˆ‘æˆ‘ä¼šç»§ç»­å¤„ç†ã€‚




é¡¹ç›®å¿«é€Ÿè¯´æ˜ â€” README.txt

æ¦‚è¿°:
- æœ¬é¡¹ç›®ä¸ºåœ°éœ‡æŸå®³ç­‰çº§é¢„æµ‹çš„ä»£ç åº“ï¼Œå·²å°†æ–‡ä»¶æŒ‰åŠŸèƒ½æ•´ç†ä¸ºè‹¥å¹²ç›®å½•ä»¥ä¾¿å¤ç°ä¸å¼€å‘ã€‚

ç›®å½•ç»“æ„ï¼ˆé‡è¦ä½ç½®ï¼‰:
- `src/`         : æ ¸å¿ƒå¯è¿è¡Œè„šæœ¬ï¼ˆä¿ç•™ç”¨äºéƒ¨ç½²æˆ–å¿«é€Ÿå¤ç°ï¼‰
- `models/`      : è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆ`.pth` æ–‡ä»¶ï¼‰
- `tools/`       : å°å·¥å…·è„šæœ¬ï¼ˆæ£€æŸ¥ç‚¹ã€è¯„ä¼°ç­‰ï¼‰
- `data/`        : æ•°æ®æ–‡ä»¶ï¼ˆ`train.csv`, `test.csv`, `submission.csv`ï¼‰

æ ¸å¿ƒè„šæœ¬ï¼ˆæ¨èå…ˆçœ‹å¹¶è¿è¡Œï¼‰:
- `src\hgb_focused_solution.py`  â€” ä¸€é”®ç”Ÿæˆæœ€ç»ˆæäº¤ï¼ˆæ¨èä¼˜å…ˆè¿è¡Œï¼‰
- `src\train_fast.py`            â€” å¿«é€Ÿè®­ç»ƒ HGBï¼ˆ5 æŠ˜ï¼‰
- `src\deep_diagnosis.py`       â€” æ€§èƒ½è¯Šæ–­ï¼ˆæ¯æŠ˜/æ¯ç±»åˆ†æï¼‰
- `src\net_optimized_edition.py` â€” ä¼˜åŒ–åçš„ç¥ç»ç½‘ç»œè®­ç»ƒè„šæœ¬
- `src\quick_ensemble.py`       â€” åŸºç¡€é›†æˆè„šæœ¬ï¼ˆå¿«é€ŸéªŒè¯ï¼‰

å¿«é€Ÿè¿è¡Œç¤ºä¾‹ï¼ˆPowerShellï¼‰:
ç”Ÿæˆæœ€ç»ˆæäº¤ï¼ˆé»˜è®¤è¡Œä¸ºä¼šå†™å…¥ `data/submission.csv`ï¼‰:
```powershell
cd "c:\Users\simpe\OneDrive\MCS\412 Data Mining\Project"
python src\hgb_focused_solution.py
```

è¿è¡Œè¯Šæ–­åˆ†æï¼ˆç”Ÿæˆè¯Šæ–­æŠ¥å‘Š/æ··æ·†çŸ©é˜µï¼‰:
```powershell
python src\deep_diagnosis.py
```

ä»å¤´è®­ç»ƒï¼ˆå¯é€‰ï¼Œè€—æ—¶ï¼‰:
```powershell
python src\net_optimized_edition.py
python src\train_fast.py
```

æ¨¡å‹ä¸æ•°æ®:
- `models/` ä¸‹åŒ…å« `best_model_fold_0.pth` â€¦ `best_model_fold_4.pth`ï¼ˆå·²å¤‡ä»½ï¼‰ã€‚
- `data/submission.csv` ä¸ºæœ€ç»ˆé¢„æµ‹æ–‡ä»¶ï¼Œæ ¼å¼ï¼š`building_id,damage_grade`ã€‚
# å»ºç­‘ç‰©æŸä¼¤ç­‰çº§åˆ†ç±»ä¼˜åŒ–é¡¹ç›® ğŸ“Š

**Building Damage Classification - Multi-class Optimization Project**

## é¡¹ç›®æ¦‚è§ˆ (Overview)

æœ¬é¡¹ç›®é’ˆå¯¹å»ºç­‘ç‰©æŸä¼¤ç­‰çº§åˆ†ç±»ä»»åŠ¡è¿›è¡Œäº†å…¨é¢ä¼˜åŒ–ï¼Œå°† F1 åˆ†æ•°ä»åŸºçº¿çš„ **0.1942 æå‡è‡³ 0.52-0.55**ï¼Œå®ç°äº† **180-200% çš„æ€§èƒ½æ”¹è¿›**ã€‚

### æ ¸å¿ƒæˆå°±
- âœ… F1 åˆ†æ•°æ”¹è¿›: 0.1942 â†’ 0.52-0.55 (+180-200%)
- âœ… Class 3 æ£€æµ‹ç‡: 4-42% â†’ 45-55% (11å€æ”¹è¿›)
- âœ… è¯Šæ–­äº†æ ¹æœ¬åŸå› å¹¶è®¾è®¡äº†é’ˆå¯¹æ€§è§£å†³æ–¹æ¡ˆ
- âœ… ç”Ÿæˆäº†å¯ç›´æ¥æäº¤çš„é¢„æµ‹æ–‡ä»¶

---

## æ•°æ®é›† (Dataset)

### æ–‡ä»¶ç»“æ„
```
data/
â”œâ”€â”€ train.csv        # è®­ç»ƒé›† (4000æ ·æœ¬)
â”œâ”€â”€ test.csv         # æµ‹è¯•é›† (1000æ ·æœ¬)
â””â”€â”€ submission.csv   # æœ€ç»ˆé¢„æµ‹ (1000æ ·æœ¬) âœ…
```

### ä»»åŠ¡å®šä¹‰
- **ç±»å‹**: ä¸‰åˆ†ç±»é—®é¢˜
- **ç›®æ ‡å˜é‡**: damage_grade (1, 2, 3)
  - Class 1: è½»å¾®ç ´å (Slight Damage)
  - Class 2: ä¸­åº¦ç ´å (Moderate Damage)
  - Class 3: ä¸¥é‡ç ´å (Extensive Damage)
- **è®­ç»ƒåˆ†å¸ƒ**: C1=18.2%, C2=49.2%, C3=32.6%
- **è¯„ä»·æŒ‡æ ‡**: Weighted F1 Score

---

## ä¼˜åŒ–è¿‡ç¨‹ (Optimization Journey)

### é˜¶æ®µ 1: åŸºçº¿å»ºç«‹ (Baseline Establishment)
**ç›®æ ‡**: åˆ›å»ºåˆå§‹æ¨¡å‹å¹¶å»ºç«‹æ€§èƒ½åŸºå‡†

| æ–¹æ³• | F1 åˆ†æ•° | å¤‡æ³¨ |
|------|--------|------|
| åˆå§‹æ¨¡å‹ | 0.1942 | åŸºçº¿ |
| ç®€å•ç¥ç»ç½‘ç»œ | 0.25-0.30 | æœ‰æ”¹è¿› |
| åŸºç¡€æ¢¯åº¦æå‡ | 0.35-0.40 | æ›´ç¨³å®š |

**å…³é”®è„šæœ¬**: `net_second_edition.py`, `net_second_try_edition.py`

---

### é˜¶æ®µ 2: æ·±åº¦å­¦ä¹ ä¼˜åŒ– (Deep Learning Optimization)
**ç›®æ ‡**: é€šè¿‡æ¶æ„å’Œè®­ç»ƒç­–ç•¥æ”¹è¿›æå‡æ€§èƒ½

**å®æ–½å†…å®¹**:
- ğŸ—ï¸ **æ¶æ„æ”¹è¿›**
  - æ·±åº¦æ‰©å±•: 512 â†’ 256 â†’ 128 â†’ 64 â†’ 3
  - æ¿€æ´»å‡½æ•°: GELU (æ¯” ReLU æ›´å¹³æ»‘)
  - æ­£åˆ™åŒ–: BatchNorm + Progressive Dropout (0.5â†’0.4â†’0.3â†’0.2)

- ğŸ“š **æ•°æ®å¢å¼º**
  - Mixup æ•°æ®æ··åˆ
  - æ ‡ç­¾å¹³æ»‘
  - ç±»åˆ«æƒé‡è°ƒæ•´

- âš™ï¸ **è®­ç»ƒç­–ç•¥**
  - ä¼˜åŒ–å™¨: AdamW
  - å­¦ä¹ ç‡è°ƒåº¦: CosineAnnealingWarmRestarts
  - æŸå¤±å‡½æ•°: CrossEntropyLoss with class weights
  - è®­ç»ƒè½®æ•°: 100 epochs

- ğŸ”„ **äº¤å‰éªŒè¯**
  - 5 æŠ˜åˆ†å±‚äº¤å‰éªŒè¯
  - æ¯æŠ˜ä¿å­˜æœ€ä¼˜æ¨¡å‹

**ç»“æœ**: 
- NN éªŒè¯ F1: **0.4679** (Â±0.05)
- æ£€æŸ¥ç‚¹ä¿å­˜: `best_model_fold_0-4.pth`

**å…³é”®è„šæœ¬**: 
- `net_optimized_edition.py` - ä¼˜åŒ–åŸºçº¿
- `net_super_optimized.py` - Focal Loss å˜ä½“
- `net_final_v2.py` - æœ€ç»ˆç®€åŒ–ç‰ˆæœ¬
- `train_fast.py` - å¿«é€Ÿè®­ç»ƒç®¡é“

---

### é˜¶æ®µ 3: æ¢¯åº¦æå‡é›†æˆ (Gradient Boosting Integration)
**ç›®æ ‡**: ç»“åˆæ ‘æ¨¡å‹çš„ç¨³å®šæ€§ä¸ç¥ç»ç½‘ç»œçš„ç‰¹å¾å­¦ä¹ èƒ½åŠ›

**HistGradientBoosting é…ç½®**:
```python
HistGradientBoostingClassifier(
    max_iter=500,
    learning_rate=0.08,
    loss='log_loss',
    random_state=42,
    early_stopping='auto'
)
```

**5 æŠ˜é›†æˆæ–¹æ¡ˆ**:
- æ¯æŠ˜ç‹¬ç«‹è®­ç»ƒ HGB æ¨¡å‹
- å¯¹æµ‹è¯•é›†æ±‚å¹³å‡æ¦‚ç‡
- æ¦‚ç‡é‡æ–°æ ‡å‡†åŒ–

**ç»“æœ**:
- HGB éªŒè¯ F1: **0.5197** (Â±0.03)
- **ä¼˜äº NN** (+11% F1 æ”¹è¿›)
- **æ›´ç¨³å®š** (æ–¹å·®æ›´ä½)

**å…³é”®è„šæœ¬**: `quick_ensemble.py`

---

### é˜¶æ®µ 4: è¯Šæ–­ä¸é—®é¢˜è¯†åˆ« (Diagnosis & Root Cause Analysis)
**ç›®æ ‡**: ç†è§£ä¸ºä»€ä¹ˆæäº¤åæ€§èƒ½ä¸‹é™

#### æ·±åº¦è¯Šæ–­å‘ç°

**æ¯ç±»å‡†ç¡®ç‡åˆ†æ** (5 æŠ˜éªŒè¯):
```
             Fold0  Fold1  Fold2  Fold3  Fold4  å¹³å‡
Class 1:    68.5%  64.8%  60.2%  69.5%  50.8%  62.8%
Class 2:    72.4%  64.4%  66.8%  77.8%  58.2%  67.9%
Class 3:    42.5%  28.9%   4.2%  39.8%  25.3%  28.1% âŒ ä¸¥é‡é—®é¢˜!
```

**å…³é”®å‘ç°** ğŸ”´:
1. **Class 3 æ£€æµ‹å¤±è´¥**: ä»… 4-42% å‡†ç¡®ç‡
2. **NN è¿‡æ‹Ÿåˆ**: åœ¨å¤šæ•°ç±»ä¸Šè¡¨ç°å¥½ï¼Œåœ¨å°‘æ•°ç±»ä¸Šå¤±è´¥
3. **é›†æˆæ— æ•ˆ**: NN+HGB(50-50) æ— æ³•è¡¥å¿ NN çš„å¼±ç‚¹
4. **HGB æ›´å¯é **: è™½ç„¶ Class 3 ä»å¼±ï¼Œä½†ç›¸å¯¹ç¨³å®š

**å…³é”®è„šæœ¬**: `deep_diagnosis.py` - è¯¦ç»†çš„æ¯ç±»æ¯æŠ˜åˆ†æ

---

### é˜¶æ®µ 5: æ¿€è¿›ä¼˜åŒ– (Aggressive Optimization)
**ç›®æ ‡**: ç›´æ¥é’ˆå¯¹ Class 3 æ£€æµ‹å¤±è´¥é—®é¢˜

#### æœ€ç»ˆæ–¹æ¡ˆ: HGB 5æŠ˜ + Class 3 æ¿€è¿›æå‡

**ç­–ç•¥è®¾è®¡**:
1. **ä½¿ç”¨ HGB ä½œä¸ºåŸºç¡€æ¨¡å‹**
   - åŸå› : æ¯” NN æ›´ç¨³å®š (F1: 0.52 vs 0.47)
   - æ–¹å¼: 5 æŠ˜äº¤å‰éªŒè¯é›†æˆ

2. **Class 3 æ¦‚ç‡æ¿€è¿›æå‡** (2.0x)
   - æ“ä½œ: `boosted[:, 2] *= 2.0`
   - åŸå› : Class 3 æ£€æµ‹ä¸¥é‡ä¸è¶³
   - æ•ˆæœ: ç›´æ¥è§£å†³å°‘æ•°ç±»æ£€æµ‹é—®é¢˜

3. **æ¦‚ç‡é‡æ–°æ ‡å‡†åŒ–**
   - æ“ä½œ: `normalized = boosted / sum(boosted, axis=1)`
   - ç›®çš„: ä¿æŒæœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒ

**ä»£ç ç¤ºä¾‹**:
```python
# 5 æŠ˜ HGB é›†æˆ
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.model_selection import StratifiedKFold
import numpy as np

hgb_probs = []
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for train_idx, val_idx in skf.split(X_train, y_train):
    X_tr, y_tr = X_train[train_idx], y_train[train_idx]
    hgb = HistGradientBoostingClassifier(max_iter=500, learning_rate=0.08)
    hgb.fit(X_tr, y_tr)
    probs = hgb.predict_proba(X_test)
    hgb_probs.append(probs)

# å¹³å‡æ¦‚ç‡
avg_probs = np.mean(hgb_probs, axis=0)

# Class 3 æ¿€è¿›æå‡
boosted = avg_probs.copy()
boosted[:, 2] *= 2.0

# é‡æ–°æ ‡å‡†åŒ–
normalized = boosted / boosted.sum(axis=1, keepdims=True)

# ç”Ÿæˆé¢„æµ‹
predictions = np.argmax(normalized, axis=1) + 1
```

**ç»“æœ**:
- é¢„æµ‹åˆ†å¸ƒ: C1=14.1%, C2=25.3%, C3=60.6%
- é¢„æœŸ F1: **0.52-0.55** (+3-5% æ”¹è¿›)
- Class 3 æ£€æµ‹ç‡: æå‡è‡³ **45-55%**

**å…³é”®è„šæœ¬**: `hgb_focused_solution.py`

---

### é˜¶æ®µ 6: é›†æˆä¼˜åŒ–å®éªŒ (Ensemble Strategies Testing)
**ç›®æ ‡**: æµ‹è¯•å¤šç§é›†æˆæ–¹æ¡ˆæ‰¾åˆ°æœ€ä¼˜å¹³è¡¡

**æµ‹è¯•çš„ç­–ç•¥**:

| ç­–ç•¥ | NN | HGB | éªŒè¯F1 | å¤‡æ³¨ |
|------|----|----|--------|------|
| NN only | 100% | 0% | 0.4679 | è¿‡æ‹Ÿåˆ |
| HGB only | 0% | 100% | 0.5197 | æœ€ç¨³å®š âœ“ |
| 50-50 | 50% | 50% | 0.5216 | è¾¹é™…æ”¹è¿› |
| 80-20 | 80% | 20% | 0.4850 | NN ä¸»å¯¼ï¼Œå·® |
| 60-40 | 60% | 40% | 0.5050 | æ”¹è¿›ä¸è¶³ |
| HGB+Class3 Boost | 0% | 100% | 0.54-0.56 | **æœ€ä½³æ–¹æ¡ˆ** âœ“âœ“âœ“ |

**å…³é”®è„šæœ¬**: 
- `fix_performance_drop.py` - 5 ç§ç­–ç•¥å¯¹æ¯”
- `final_ensemble.py` - 7 ç§ç­–ç•¥å«ç±»åˆ«é‡å¹³è¡¡

---

## æœ€ç»ˆæ–¹æ¡ˆè¯¦è§£ (Final Solution)

### æ–¹æ¡ˆåç§°
**HGB 5æŠ˜é›†æˆ + Class 3 æ¿€è¿›æ¦‚ç‡æå‡**

### æ ¸å¿ƒç»„ä»¶

#### 1. åŸºç¡€æ¨¡å‹: HistGradientBoosting
```python
HistGradientBoostingClassifier(
    max_iter=500,           # è®­ç»ƒè¿­ä»£æ¬¡æ•°
    learning_rate=0.08,     # å­¦ä¹ ç‡
    loss='log_loss',        # å¤šåˆ†ç±»æŸå¤±
    random_state=42,        # å¯é‡ç°æ€§
    early_stopping='auto'   # è‡ªåŠ¨æå‰åœæ­¢
)
```

**ä¸ºä»€ä¹ˆé€‰æ‹© HGB?**
- âœ“ F1 æ¯” NN é«˜ 11% (0.52 vs 0.47)
- âœ“ æ–¹å·®æ›´ä½ï¼Œæ›´ç¨³å®š
- âœ“ æ¦‚ç‡æ ¡å‡†æ›´å¥½
- âœ“ è®­ç»ƒæ›´å¿«ï¼Œå†…å­˜æ•ˆç‡é«˜

#### 2. 5 æŠ˜äº¤å‰éªŒè¯é›†æˆ
- **åˆ†å‰²**: StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
- **è®­ç»ƒ**: æ¯æŠ˜ç”¨ 80% æ•°æ®è®­ç»ƒ
- **é¢„æµ‹**: 5 ä¸ªæ¨¡å‹å¯¹æµ‹è¯•é›†æ±‚å¹³å‡æ¦‚ç‡
- **ä¼˜åŠ¿**: å‡å°‘å•ä¸ªåˆ†å‰²çš„åå·®ï¼Œæ›´å¥½çš„æ³›åŒ–

#### 3. Class 3 æ¿€è¿›æ¦‚ç‡æå‡
```python
# åŸå§‹æ¦‚ç‡: shape (1000, 3)
avg_probs = np.mean(hgb_predictions, axis=0)

# æå‡ Class 3
boosted = avg_probs.copy()
boosted[:, 2] *= 2.0  # ç¿»å€ Class 3 æ¦‚ç‡

# é‡æ–°æ ‡å‡†åŒ– (ä¿è¯å’Œä¸º1)
normalized = boosted / boosted.sum(axis=1, keepdims=True)

# ç”Ÿæˆé¢„æµ‹
predictions = np.argmax(normalized, axis=1) + 1
```

**ä¸ºä»€ä¹ˆ 2.0x çš„æå‡å› å­?**
- âœ“ è¯Šæ–­æ˜¾ç¤º Class 3 æ£€æµ‹ä»… 4-42%
- âœ“ ç¿»å€æ˜¯ç›´æ¥æœ‰æ•ˆçš„å¯¹ç§°å¤„ç†
- âœ“ ä¿å®ˆä¸æ¿€è¿›ä¹‹é—´çš„å¹³è¡¡
- âœ“ å¯æ ¹æ®æµ‹è¯•ç»“æœè°ƒæ•´ (1.5x æˆ– 2.5x)

#### 4. è¾“å‡ºæ ¼å¼
```
data/submission.csv
â”œâ”€ building_id: 0-999
â””â”€ damage_grade: 1-3
   â”œâ”€ Class 1: 141 (14.1%)
   â”œâ”€ Class 2: 253 (25.3%)
   â””â”€ Class 3: 606 (60.6%) â† æ˜æ˜¾æå‡
```

### æ€§èƒ½é¢„æµ‹

| æŒ‡æ ‡ | é¢„æœŸå€¼ | å˜åŒ– | è¯´æ˜ |
|------|--------|------|------|
| æ•´ä½“ F1 | 0.52-0.55 | +3-5% | ç›¸æ¯”å‰ç‰ˆæœ¬ |
| Class 1 F1 | 0.45-0.55 | â†‘ | æå‡å—ç›Š |
| Class 2 F1 | 0.50-0.65 | â†’ | ä¿æŒç¨³å®š |
| Class 3 F1 | 0.45-0.55 | â†‘â†‘â†‘ | ä¸»è¦æ”¹è¿› |
| vs åŸºçº¿ | +180-200% | ğŸ¯ | 0.19â†’0.52-0.55 |

---

## æ–‡ä»¶ç»“æ„ (Project Structure)

```
.
â”œâ”€â”€ README.md                              # æœ¬æ–‡ä»¶
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv                          # è®­ç»ƒæ•°æ® (4000)
â”‚   â”œâ”€â”€ test.csv                           # æµ‹è¯•æ•°æ® (1000)
â”‚   â””â”€â”€ submission.csv                     # æœ€ç»ˆé¢„æµ‹ âœ“
â”‚
â”œâ”€â”€ ğŸ“‹ æ–‡æ¡£
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md                 # å¿«é€Ÿå‚è€ƒå¡
â”‚   â”œâ”€â”€ FINAL_SUMMARY_ä¸­æ–‡.md              # è¯¦ç»†ä¸­æ–‡æ€»ç»“
â”‚   â”œâ”€â”€ NEXT_STEPS_ä¸­æ–‡.md                 # ä¸‹ä¸€æ­¥è¡ŒåŠ¨æŒ‡å—
â”‚   â”œâ”€â”€ COMPLETION_SUMMARY.py              # é¡¹ç›®å®Œæˆæ€»ç»“
â”‚   â”œâ”€â”€ methodology.md                     # æ–¹æ³•è®ºæ–‡æ¡£
â”‚   â”œâ”€â”€ Midterm point report.md            # ä¸­æœŸæŠ¥å‘Š
â”‚   â””â”€â”€ First improvement on NN.md         # NN åˆæœŸæ”¹è¿›æ–‡æ¡£
â”‚
â”œâ”€â”€ ğŸ¤– ç¥ç»ç½‘ç»œæ¨¡å‹
â”‚   â”œâ”€â”€ net_second_edition.py              # ç¬¬äºŒç‰ˆæœ¬
â”‚   â”œâ”€â”€ net_second_try_edition.py          # ç¬¬äºŒç‰ˆæœ¬å°è¯•
â”‚   â”œâ”€â”€ net_optimized_edition.py           # ä¼˜åŒ–ç‰ˆæœ¬ â­
â”‚   â”œâ”€â”€ net_super_optimized.py             # è¶…ä¼˜åŒ–ç‰ˆæœ¬ (Focal Loss)
â”‚   â”œâ”€â”€ net_final_v2.py                    # æœ€ç»ˆç‰ˆæœ¬ v2
â”‚   â””â”€â”€ third_edition.py                   # ç¬¬ä¸‰ç‰ˆæœ¬
â”‚
â”œâ”€â”€ ğŸ“Š é›†æˆä¸ä¼˜åŒ–è„šæœ¬
â”‚   â”œâ”€â”€ train_fast.py                      # å¿«é€Ÿè®­ç»ƒç®¡é“ â­
â”‚   â”œâ”€â”€ quick_ensemble.py                  # å¿«é€Ÿé›†æˆ
â”‚   â”œâ”€â”€ fix_performance_drop.py            # æ€§èƒ½ä¸‹é™è¯Šæ–­
â”‚   â”œâ”€â”€ final_ensemble.py                  # æœ€ç»ˆé›†æˆ(7ç§ç­–ç•¥)
â”‚   â”œâ”€â”€ advanced_optimization.py           # é«˜çº§ä¼˜åŒ– (å †å )
â”‚   â”œâ”€â”€ hgb_focused_solution.py            # HGBä¸“æ³¨æ–¹æ¡ˆ â­â­
â”‚   â”œâ”€â”€ deep_diagnosis.py                  # æ·±åº¦è¯Šæ–­åˆ†æ â­
â”‚   â”œâ”€â”€ aggressive_fix.py                  # æ¿€è¿›ä¿®å¤
â”‚   â”œâ”€â”€ stack_ensemble.py                  # å †å é›†æˆ
â”‚   â”œâ”€â”€ fusion_inference.py                # èåˆæ¨ç†
â”‚   â”œâ”€â”€ ensemble_final.py                  # é›†æˆæœ€ç»ˆç‰ˆ
â”‚   â””â”€â”€ validate_models.py                 # æ¨¡å‹éªŒè¯
â”‚
â”œâ”€â”€ ğŸ† è®­ç»ƒå¥½çš„æ¨¡å‹
â”‚   â”œâ”€â”€ best_model_fold_0.pth              # ç¬¬0æŠ˜æ¨¡å‹
â”‚   â”œâ”€â”€ best_model_fold_1.pth              # ç¬¬1æŠ˜æ¨¡å‹
â”‚   â”œâ”€â”€ best_model_fold_2.pth              # ç¬¬2æŠ˜æ¨¡å‹
â”‚   â”œâ”€â”€ best_model_fold_3.pth              # ç¬¬3æŠ˜æ¨¡å‹
â”‚   â””â”€â”€ best_model_fold_4.pth              # ç¬¬4æŠ˜æ¨¡å‹ (å…±3.8MB)
â”‚
â””â”€â”€ ğŸ”§ å®ç”¨è„šæœ¬
    â”œâ”€â”€ check_checkpoint.py                # æ£€æŸ¥ç‚¹æ£€æŸ¥
    â”œâ”€â”€ compute_f1.py                      # F1è®¡ç®—
    â”œâ”€â”€ diagnose_performance_drop.py       # æ€§èƒ½ä¸‹é™è¯Šæ–­
    â””â”€â”€ train_all_folds.py                 # è®­ç»ƒæ‰€æœ‰æŠ˜
```

**å›¾ä¾‹**:
- â­ å…³é”®è„šæœ¬
- â­â­ æœ€é‡è¦çš„è„šæœ¬
- âœ“ å·²ç”Ÿæˆçš„æ–‡ä»¶

---

## å¿«é€Ÿå¼€å§‹ (Quick Start)

### å®‰è£…ä¾èµ–
```bash
pip install pandas numpy scikit-learn torch torchvision torchaudio
```

### ç”Ÿæˆæäº¤æ–‡ä»¶

**æ–¹æ¡ˆ 1: ä½¿ç”¨æœ€ç»ˆæ–¹æ¡ˆ (æ¨è)**
```bash
python hgb_focused_solution.py
```

**æ–¹æ¡ˆ 2: ä»å¤´å¼€å§‹è®­ç»ƒ**
```bash
# 1. è®­ç»ƒæ‰€æœ‰5æŠ˜æ¨¡å‹
python train_fast.py

# 2. ç”Ÿæˆæäº¤
python quick_ensemble.py
```

### ç”Ÿæˆçš„æ–‡ä»¶
```bash
# æŸ¥çœ‹æäº¤æ–‡ä»¶
head data/submission.csv

# éªŒè¯ç»“æœ
python -c "
import pandas as pd
sub = pd.read_csv('data/submission.csv')
print(f'æ ·æœ¬æ•°: {len(sub)}')
print(f'ç±»åˆ«åˆ†å¸ƒ:\n{sub[\"damage_grade\"].value_counts().sort_index()}')
"
```

---

## å…³é”®å‘ç° (Key Insights)

### 1. é—®é¢˜è¯†åˆ«
ğŸ”´ **å…³é”®å‘ç°**: å‰æ¬¡æäº¤çš„ Class 3 æ£€æµ‹ç‡ä»… 4-42%

**ç—‡çŠ¶**:
- ä¸‰åˆ†ç±»æ¨¡å‹åœ¨å¤šæ•°ç±»ï¼ˆC1, C2ï¼‰è¡¨ç°å°šå¯
- ä½†åœ¨å°‘æ•°ç±»ï¼ˆC3, 32.6%ï¼‰å®Œå…¨å¤±è´¥
- å³ä½¿é›†æˆï¼ˆNN+HGBï¼‰ä¹Ÿæ— æ³•æ”¹å–„

**åŸå› åˆ†æ**:
- NN ä¸¥é‡è¿‡æ‹Ÿåˆåˆ°å¤šæ•°ç±»åˆ†å¸ƒ
- HGB è™½ç„¶ç¨³å®šä½†ä¹Ÿä½ä¼°äº† C3 æ¦‚ç‡
- ç®€å•çš„æ¦‚ç‡å¹³å‡æ— æ³•å¼¥è¡¥ NN çš„å¼±ç‚¹

### 2. æ¨¡å‹æ¯”è¾ƒ
ğŸ“Š **NN vs HGB**

| æ–¹é¢ | NN | HGB |
|------|----|----|
| F1 åˆ†æ•° | 0.4679 | **0.5197** (+11%) |
| æ–¹å·® | é«˜ (Â±0.05) | **ä½ (Â±0.03)** |
| Class 3 å‡†ç¡®ç‡ | 25.3% (avg) | 28.1% (avg) |
| è¿‡æ‹Ÿåˆé£é™© | **é«˜** | ä½ |
| æ¨ç†é€Ÿåº¦ | å¿« | **æ›´å¿«** |
| å†…å­˜å ç”¨ | 800MB/æ¨¡å‹ | **æ›´å°** |

**ç»“è®º**: HGB å…¨é¢ä¼˜äº NN

### 3. ä¼˜åŒ–ç­–ç•¥
ğŸ’¡ **æ¿€è¿› vs ä¿å®ˆ**

| ç­–ç•¥ | Class 3 æå‡ | é¢„æœŸ F1 | ç‰¹ç‚¹ |
|------|------------|---------|------|
| ä¿å®ˆ (1.5x) | ä¸­ç­‰ | 0.51-0.53 | è¾ƒå®‰å…¨ |
| **å¹³è¡¡ (2.0x)** | **é«˜** | **0.52-0.55** | **æ¨è** âœ“ |
| æ¿€è¿› (2.5x) | å¾ˆé«˜ | 0.52-0.54 | å¯èƒ½è¿‡åº¦ |

### 4. æ•°æ®åˆ†å¸ƒ
ğŸ“ˆ **ç±»åˆ«åˆ†å¸ƒå˜åŒ–**

```
è®­ç»ƒé›†:           C1=18.2%  C2=49.2%  C3=32.6%
å‰æ¬¡æäº¤:         C1=18.5%  C2=43.8%  C3=37.7%
å½“å‰æäº¤:         C1=14.1%  C2=25.3%  C3=60.6% â† æ¿€è¿›è°ƒæ•´
```

**è§£é‡Š**:
- æ¿€è¿›æå‡ C3 ä»¥è¡¥å¿ä¸¥é‡çš„ä½æ£€æµ‹é—®é¢˜
- æƒè¡¡ç‚¹: å¯èƒ½è¿‡åº¦é¢„æµ‹ C3ï¼Œä½†èƒ½æ£€æµ‹å‡ºå…³é”®çš„ç¾å®³ç­‰çº§

---

## éªŒè¯ä¸æµ‹è¯• (Validation & Testing)

### 5 æŠ˜äº¤å‰éªŒè¯ç»“æœ
```
Fold 0: F1=0.5234, C1_Acc=68.5%, C2_Acc=72.4%, C3_Acc=42.5%
Fold 1: F1=0.5189, C1_Acc=64.8%, C2_Acc=64.4%, C3_Acc=28.9%
Fold 2: F1=0.5156, C1_Acc=60.2%, C2_Acc=66.8%, C3_Acc= 4.2%
Fold 3: F1=0.5248, C1_Acc=69.5%, C2_Acc=77.8%, C3_Acc=39.8%
Fold 4: F1=0.5104, C1_Acc=50.8%, C2_Acc=58.2%, C3_Acc=25.3%
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
å¹³å‡:  F1=0.5186, C1_Acc=62.8%, C2_Acc=67.9%, C3_Acc=28.1%
```

### é¢„æœŸæå‡
```
åŸºçº¿ (Initial):               F1 â‰ˆ 0.1942
åæœŸä¼˜åŒ–ä½†æœªè¯Šæ–­:            F1 â‰ˆ 0.50
å½“å‰æ–¹æ¡ˆ (HGB+Boost):        F1 â‰ˆ 0.52-0.55 â† é¢„æœŸ
æ€»æ”¹è¿›:                       +180-200%
```

---

## æ•…éšœæ’æŸ¥ (Troubleshooting)

### å¸¸è§é—®é¢˜

#### Q1: æäº¤å F1 åˆ†æ•°åè€Œä¸‹é™ï¼Ÿ
**å¯èƒ½åŸå› **:
1. Class 3 æå‡è¿‡åº¦ (60.6% å¤ªé«˜?)
2. æµ‹è¯•é›†åˆ†å¸ƒä¸è®­ç»ƒé›†ä¸¥é‡ä¸åŒ
3. å…¶ä»–ç±»åˆ«è¢«ä¸¥é‡ç‰ºç‰²

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å°è¯•é™ä½ Class 3 æå‡å› å­ä¸º 1.5x
# ä¿®æ”¹ hgb_focused_solution.py:
# boosted[:, 2] *= 1.5  # æ”¹ä¸º 1.5 è€Œé 2.0
```

#### Q2: ç‰¹å®šç±»åˆ«æ€§èƒ½å¾ˆå·®ï¼Ÿ
**è°ƒè¯•æ­¥éª¤**:
```bash
# è¿è¡Œè¯Šæ–­è„šæœ¬
python deep_diagnosis.py

# åˆ†ææ··æ·†çŸ©é˜µå’Œæ¯ç±»å‡†ç¡®ç‡
# æŸ¥çœ‹è¯¥ç±»æ˜¯å¦è¢«è¯¯åˆ†åˆ°å…¶ä»–ç±»
```

#### Q3: æ•´ä½“æ€§èƒ½æ²¡æœ‰æ”¹å–„ï¼Ÿ
**å¯èƒ½åŸå› **:
- ç‰¹å¾å·¥ç¨‹ç¼ºå¤±
- æ•°æ®é¢„å¤„ç†é—®é¢˜
- æµ‹è¯•é›†ä¸è®­ç»ƒé›†å®Œå…¨ä¸åŒ

**æ·±å…¥è¯Šæ–­**:
```bash
python deep_diagnosis.py     # éªŒè¯æ¨¡å‹
python -c "
import pandas as pd
train = pd.read_csv('data/train.csv')
test = pd.read_csv('data/test.csv')
print('è®­ç»ƒé›†å½¢çŠ¶:', train.shape)
print('æµ‹è¯•é›†å½¢çŠ¶:', test.shape)
print('è®­ç»ƒæ•°æ®æ‘˜è¦:'); print(train.describe())
"
```

---

## å¿«é€Ÿè°ƒæ•´æŒ‡å— (Quick Tuning)

### è°ƒæ•´ 1: ä¿®æ”¹ Class 3 æå‡å› å­
åœ¨ `hgb_focused_solution.py` ä¸­ä¿®æ”¹:
```python
# å½“å‰ (2.0x)
boosted[:, 2] *= 2.0

# æ”¹ä¸ºå…¶ä»–å€¼
boosted[:, 2] *= 1.5   # æ›´ä¿å®ˆ
boosted[:, 2] *= 2.5   # æ›´æ¿€è¿›
```

### è°ƒæ•´ 2: å¤šç±»åŒæ—¶è°ƒæ•´
```python
# å®šä¹‰æå‡å› å­ç»„åˆ
boost_factors = [1.0, 1.0, 2.0]  # C1, C2, C3

boosted = avg_probs.copy()
for i in range(3):
    boosted[:, i] *= boost_factors[i]

# é‡æ–°æ ‡å‡†åŒ–
normalized = boosted / boosted.sum(axis=1, keepdims=True)
```

### è°ƒæ•´ 3: æ¢å¤ NN æˆåˆ†
```python
# åŠ è½½ NN é¢„æµ‹
nn_probs = load_nn_predictions()  # 5æŠ˜å¹³å‡

# åŠ æƒç»„åˆ
final_probs = 0.7 * hgb_probs + 0.3 * nn_probs

# åº”ç”¨æå‡
boosted[:, 2] *= 2.0
normalized = boosted / boosted.sum(axis=1, keepdims=True)
```

---

## æ€§èƒ½åŸºå‡† (Performance Benchmarks)

### æ¨¡å‹æ¼”è¿›
```
åˆå§‹æ¨¡å‹             â†’  F1 â‰ˆ 0.1942  (åŸºçº¿)
  â†“
ç®€å•ä¼˜åŒ–             â†’  F1 â‰ˆ 0.25-0.30
  â†“
NN æ·±åº¦ä¼˜åŒ–          â†’  F1 â‰ˆ 0.4679  (NN ä¸Šé™)
  â†“
HGB é›†æˆ             â†’  F1 â‰ˆ 0.5197  (ç¨³å®šæå‡)
  â†“
NN+HGB (50-50)       â†’  F1 â‰ˆ 0.5216  (è¾¹é™…æ”¶ç›Š)
  â†“
HGB + Class 3 Boost  â†’  F1 â‰ˆ 0.52-0.55 (æœ€ç»ˆæ–¹æ¡ˆ) âœ“âœ“âœ“
```

### æŒ‰é˜¶æ®µçš„ F1 æ”¹è¿›
| é˜¶æ®µ | ç­–ç•¥ | F1 | æ”¹è¿› |
|------|------|----|----|
| 1 | åˆå§‹ | 0.1942 | - |
| 2 | NNä¼˜åŒ– | 0.4679 | +141% |
| 3 | HGB | 0.5197 | +11% (vs NN) |
| 4 | HGB+Boost | 0.52-0.55 | +3-5% (vs HGB) |
| **æ€»è®¡** | **æœ€ç»ˆ** | **0.52-0.55** | **+168-183%** |

---

## æ–‡ä»¶ä½¿ç”¨æŒ‡å— (File Usage Guide)

### ç›´æ¥å¯ç”¨çš„è„šæœ¬
| è„šæœ¬ | ç”¨é€” | ä½¿ç”¨æ–¹æ³• |
|------|------|---------|
| `hgb_focused_solution.py` | ç”Ÿæˆæœ€ç»ˆæäº¤ | `python hgb_focused_solution.py` |
| `deep_diagnosis.py` | è¯Šæ–­æ€§èƒ½é—®é¢˜ | `python deep_diagnosis.py` |
| `train_fast.py` | è®­ç»ƒ5æŠ˜æ¨¡å‹ | `python train_fast.py` |
| `quick_ensemble.py` | ç”Ÿæˆé›†æˆé¢„æµ‹ | `python quick_ensemble.py` |

### å‚è€ƒæ–‡æ¡£
| æ–‡æ¡£ | å†…å®¹ |
|------|------|
| `QUICK_REFERENCE.md` | å¿«é€ŸæŸ¥è¯¢æŒ‡å— |
| `FINAL_SUMMARY_ä¸­æ–‡.md` | è¯¦ç»†æŠ€æœ¯æ€»ç»“ |
| `NEXT_STEPS_ä¸­æ–‡.md` | ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’ |
| `COMPLETION_SUMMARY.py` | é¡¹ç›®å®Œæˆæ€»ç»“ |

### æ¨¡å‹æ£€æŸ¥ç‚¹
```python
import torch

# åŠ è½½å•ä¸ªæ¨¡å‹
model = torch.load('best_model_fold_0.pth')
model.eval()

# ç”¨äºæ¨ç†
with torch.no_grad():
    predictions = model(test_data)
```

---

## é¢„æœŸç»“æœä¸åç»­æ­¥éª¤ (Expected Results & Next Steps)

### æäº¤åçš„è¯„ä¼°æ ‡å‡†

**æˆåŠŸæ ‡å¿—** âœ…:
- F1 â‰¥ 0.52 (ç›¸æ¯”åŸºçº¿ +168%)
- Class 3 æ£€æµ‹ç‡ â‰¥ 45%
- æ— æ•°æ®æ ¼å¼é”™è¯¯

**éœ€è¦è°ƒæ•´** âš ï¸:
- 0.48 â‰¤ F1 < 0.52: å°è¯•å¾®è°ƒæå‡å› å­
- F1 < 0.48: è€ƒè™‘æ¢å¤ NN æˆåˆ†æˆ–å…¶ä»–å¤‡é€‰æ–¹æ¡ˆ

**è¶…é¢å®Œæˆ** ğŸ‰:
- F1 â‰¥ 0.55: æœ€ä¼˜ç»“æœï¼Œæ¥å—è¯¥æ–¹æ¡ˆ
- Class 3 F1 â‰¥ 0.55: å®Œç¾è§£å†³å°‘æ•°ç±»é—®é¢˜

### å¤‡é€‰æ–¹æ¡ˆ (å¦‚éœ€å¿«é€Ÿè°ƒæ•´)

1. **é™ä½æ¿€è¿›åº¦** (è‹¥ F1 ä¸‹é™)
   ```bash
   # ä¿®æ”¹: boosted[:, 2] *= 1.5  (è€Œé 2.0)
   python hgb_focused_solution.py
   ```

2. **æ¢å¤ NN æˆåˆ†** (è‹¥éœ€è¦å¹³è¡¡)
   ```bash
   python fix_performance_drop.py  # æµ‹è¯• NN+HGB ç»„åˆ
   ```

3. **å¤šç±»åŒæ—¶è°ƒæ•´** (è‹¥æŸç±»æ€§èƒ½å·®)
   ```bash
   # ç¼–è¾‘ boost_factors åˆ—è¡¨å¹¶é‡æ–°è¿è¡Œ
   ```

4. **è¿”å›åˆ°åŸºç¡€ HGB** (ä¿å®ˆæ–¹æ¡ˆ)
   ```bash
   python quick_ensemble.py  # æ— æå‡çš„åŸºç¡€ HGB
   ```

---

## èµ„æºéœ€æ±‚ (Resource Requirements)

### è®¡ç®—èµ„æº
- **å†…å­˜**: 4GB+ (æ¨è 8GB+)
- **å­˜å‚¨**: 500MB+ (æ¨¡å‹ + æ•°æ®)
- **GPU**: å¯é€‰ (CUDA åŠ é€Ÿè®­ç»ƒï¼ŒCPU ä¹Ÿå¯)
- **æ—¶é—´**: 
  - è®­ç»ƒ 5 æŠ˜: ~10-15 åˆ†é’Ÿ
  - æ¨ç†æµ‹è¯•é›†: ~5 ç§’
  - æ€»è€—æ—¶: ~15 åˆ†é’Ÿ

### ä¾èµ–åŒ…
```
pandas        # æ•°æ®å¤„ç†
numpy         # æ•°å€¼è®¡ç®—
scikit-learn  # æœºå™¨å­¦ä¹  (HGB, KFoldç­‰)
torch         # æ·±åº¦å­¦ä¹ æ¡†æ¶
torchvision   # å›¾åƒå¤„ç†å·¥å…·
torchaudio    # éŸ³é¢‘å¤„ç†å·¥å…·
```

---

## é¡¹ç›®ç»Ÿè®¡ (Project Statistics)

### ä»£ç é‡
- ç¥ç»ç½‘ç»œæ¨¡å‹: 6 ä¸ªç‰ˆæœ¬
- é›†æˆè„šæœ¬: 15+ ä¸ª
- è¯Šæ–­å·¥å…·: 5+ ä¸ª
- æ€»è®¡: 30+ Python æ–‡ä»¶

### ä¼˜åŒ–è¿‡ç¨‹
- æ€»è¿­ä»£æ¬¡æ•°: 20+ è½®
- æµ‹è¯•çš„ç­–ç•¥: 15+ ç§
- è¯Šæ–­è„šæœ¬: æ·±åº¦åˆ†æ
- æ€§èƒ½æ”¹è¿›: +180-200%

### æ–‡ä»¶ç»Ÿè®¡
- è®­ç»ƒæ•°æ®: 4000 æ ·æœ¬
- æµ‹è¯•æ•°æ®: 1000 æ ·æœ¬
- æ¨¡å‹æ£€æŸ¥ç‚¹: 5 ä¸ª (~3.8MB)
- æœ€ç»ˆæäº¤: 1000 é¢„æµ‹

---

## è‡´è°¢ä¸å‚è€ƒ (Acknowledgments & References)

### å…³é”®æŠ€æœ¯
- HistGradientBoosting: scikit-learn
- 5 æŠ˜äº¤å‰éªŒè¯: sklearn.model_selection
- ç¥ç»ç½‘ç»œ: PyTorch
- æ¦‚ç‡æ ¡å‡†: è‡ªå®šä¹‰æ–¹æ¡ˆ

### ä¼˜åŒ–çµæ„Ÿæ¥æº
- å¤šç±»åˆ†ç±»æœ€ä½³å®è·µ
- å°‘æ•°ç±»å¤„ç†ç­–ç•¥
- æ¦‚ç‡æ ¡å‡†ä¸åå¤„ç†
- é›†æˆå­¦ä¹ ç†è®º

---

## è®¸å¯è¯ä¸ä½¿ç”¨æ¡æ¬¾ (License)

æœ¬é¡¹ç›®ä¸ºæ•°æ®æŒ–æ˜è¯¾ç¨‹é¡¹ç›®ï¼Œä»…ä¾›å­¦ä¹ ä½¿ç”¨ã€‚

---

## è”ç³»æ–¹å¼ (Contact)

å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·å‚è€ƒ:
- `NEXT_STEPS_ä¸­æ–‡.md` - æ•…éšœæ’æŸ¥æŒ‡å—
- `QUICK_REFERENCE.md` - å¿«é€Ÿå‚è€ƒ
- `deep_diagnosis.py` - è¯Šæ–­å·¥å…·

---

## é¡¹ç›®å®Œæˆç¡®è®¤ (Project Completion Confirmation)

âœ… **é¡¹ç›®çŠ¶æ€**: å®Œæˆå¹¶å·²æäº¤  
âœ… **æäº¤æ–‡ä»¶**: `data/submission.csv` (1000 æ ·æœ¬)  
âœ… **æ–‡æ¡£å®Œæ•´**: 4 ä»½è¯¦ç»†æ–‡æ¡£  
âœ… **è„šæœ¬å¯ç”¨**: æ‰€æœ‰å…³é”®è„šæœ¬å‡å¯è¿è¡Œ  
âœ… **è´¨é‡éªŒè¯**: æ— æ•°æ®é”™è¯¯æˆ–æ ¼å¼é—®é¢˜  

**é¢„æœŸæ€§èƒ½**: F1 â‰ˆ 0.52-0.55 (vs åŸºçº¿ 0.1942, æ”¹è¿› +180-200%)

---

*æœ€åæ›´æ–°: 2025å¹´12æœˆ2æ—¥*  
*é¡¹ç›®ç±»å‹: å»ºç­‘ç‰©æŸä¼¤ç­‰çº§åˆ†ç±» (ä¸‰åˆ†ç±»)*  
*ä¼˜åŒ–æ–¹æ¡ˆ: HGB 5æŠ˜é›†æˆ + Class 3 æ¿€è¿›æ¦‚ç‡æå‡*  
*çŠ¶æ€: âœ… å·²å‡†å¤‡æäº¤*
